<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Hanzra Tech</title>
    <description>Computer Vision and Programming blog
</description>
    <link>http://bikz05.github.io/</link>
    <atom:link href="http://bikz05.github.io/feed.xml" rel="self" type="application/rss+xml" />
    <pubDate>Sat, 16 Jul 2016 00:23:12 -0700</pubDate>
    <lastBuildDate>Sat, 16 Jul 2016 00:23:12 -0700</lastBuildDate>
    <generator>Jekyll v2.5.3</generator>
    
      <item>
        <title>PCA Vectorized Derivation</title>
        <description>&lt;p&gt;In PCA, we want to find a set of orthogonal basis vectors that maximize variance of the projections of the data on them. Let \( \mathbf{X} \) be the \( n \times d \) matrix where each row is a data point and \( \mathbf{w} \) be the \( d \times 1 \) dimension basis vector. The optimization problem can be formulated as –&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;J(\mathbf{w}) = \arg \max_{\mathbf{w}} ||\mathbf{X \cdot w}||^2 \text{subjected to} ||\mathbf{w}|| = 1&lt;/script&gt;

&lt;p&gt;Introducing &lt;a href=&quot;https://en.wikipedia.org/wiki/Lagrange_multiplier&quot;&gt;Lagrange multiplier&lt;/a&gt; \( \lambda \) we get –&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;J(\mathbf{w}) = \arg \max_{\mathbf{w}} ||\mathbf{X \cdot w}||^2 + \lambda(1 - ||\mathbf{w}||^2)&lt;/script&gt;

&lt;p&gt;Rewriting the norm in terms of dot product –&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;J(\mathbf{w}) = \arg \max_{\mathbf{w}} \mathbf{(X \cdot w)^\intercal } \cdot \mathbf{(X \cdot w)} + \lambda(1-\mathbf{w^\intercal} \cdot \mathbf{w})&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;J(\mathbf{w}) = \arg \max_{\mathbf{w}} \mathbf{w^\intercal \cdot X^\intercal \cdot X \cdot w} + \lambda(1-\mathbf{w^\intercal} \cdot \mathbf{w})&lt;/script&gt;

&lt;p&gt;The Jacobian \( \nabla J(\mathbf{w}) \) is given by –&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\nabla J(\mathbf{w}) =  \mathbf{(X^\intercal X \cdot w)}^\intercal + \mathbf{w^\intercal \cdot X^\intercal \cdot X} -\lambda \mathbf{w^\intercal} - \lambda \mathbf{w^\intercal}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\nabla J(\mathbf{w}) =  2\mathbf{(X^\intercal X \cdot w)}^\intercal -2\lambda \mathbf{w^\intercal}&lt;/script&gt;

&lt;p&gt;The maxima will occur when \( \nabla J(\mathbf{w}) = 0 \).&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;2\mathbf{(X^\intercal X \cdot w)}^\intercal -2\lambda \mathbf{w^\intercal} = 0&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbf{(X^\intercal X \cdot w)}^\intercal = \lambda \mathbf{w^\intercal}&lt;/script&gt;

&lt;p&gt;Taking transpose on both sides we get –&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbf{X^\intercal X \cdot w} = \lambda \mathbf{w} \label{result}&lt;/script&gt;

&lt;p&gt;From the last equation, it is easy to see that the orthogonal basis vectors are the eigenvectors of the scatter matrix \( \mathbf{(X^\intercal X)} \) . This can be conveniently implemented in MATLAB using –&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;eigvec&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;eigval&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;eig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;#39;*X)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;where &lt;code&gt;eigvec&lt;/code&gt; are the eigenvectors and &lt;code&gt;eigval&lt;/code&gt; are the eigenvectors.&lt;/p&gt;

&lt;p&gt;Using PCA, we will get a set of atmost \( d \) eigenvectors with their corresponding non-zero eigenvalue. The eigenvalue represents how much information about the data is represented by a principal component. The greater the eigenvalue – the more information the principal component contains.&lt;/p&gt;

&lt;h1 id=&quot;resources&quot;&gt;Resources&lt;/h1&gt;

&lt;p&gt;LaTeXed notes of the blog post are available in –&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/bikz05/latexed-notes/raw/master/machine-learning/pca.tex&quot;&gt;.tex format&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/bikz05/latexed-notes/raw/master/machine-learning/pca.pdf&quot;&gt;.pdf format&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Sat, 16 Jul 2016 00:00:00 -0700</pubDate>
        <link>http://bikz05.github.io/2016/07/16/pca-vectorized-derivation.html</link>
        <guid isPermaLink="true">http://bikz05.github.io/2016/07/16/pca-vectorized-derivation.html</guid>
        
        <category>pca</category>
        
        <category>machine learning</category>
        
        <category>dimensionality reduction</category>
        
        <category>derivation</category>
        
        
        <category>pca</category>
        
        <category>machine learning</category>
        
        <category>dimensionality reduction</category>
        
        <category>derivation</category>
        
      </item>
    
      <item>
        <title>A Gentle Introduction to Robotic Grasp Planning</title>
        <description>&lt;p&gt;I am sharing the notes written by my team on &lt;strong&gt;&lt;em&gt;A gentle Introduction to Grasp Planning&lt;/em&gt;&lt;/strong&gt; for the Robot Autonomy class taught by Professor &lt;a href=&quot;https://twitter.com/siddhss5&quot;&gt;Siddhartha Srinivasa&lt;/a&gt; at CMU.&lt;/p&gt;

&lt;p&gt;Our emphasis was on provide an intuitive introduction to the reader without getting bogged down by the intricate mathematics.&lt;/p&gt;

&lt;p&gt;The PDF file can be downloaded from &lt;a href=&quot;http://personalrobotics.ri.cmu.edu/files/courses/16662/scribe/scribe_lec6.pdf&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p style=&quot; margin: 12px auto 6px auto; font-family: Helvetica,Arial,Sans-serif; font-style: normal; font-variant: normal; font-weight: normal; font-size: 14px; line-height: normal; font-size-adjust: none; font-stretch: normal; -x-system-font: none; display: block;&quot;&gt;   &lt;a title=&quot;View A Gentle Introduction to Grasp Planning on Scribd&quot; href=&quot;https://www.scribd.com/doc/303456513/A-Gentle-Introduction-to-Grasp-Planning&quot; style=&quot;text-decoration: underline;&quot;&gt;A Gentle Introduction to Grasp Planning&lt;/a&gt; by &lt;a title=&quot;View BikramjotSingh&#39;s profile on Scribd&quot; href=&quot;https://www.scribd.com/user/52903180/BikramjotSingh&quot; style=&quot;text-decoration: underline;&quot;&gt;BikramjotSingh&lt;/a&gt;&lt;/p&gt;
&lt;iframe class=&quot;scribd_iframe_embed&quot; src=&quot;https://www.scribd.com/embeds/303456513/content?start_page=1&amp;amp;view_mode=scroll&amp;amp;access_key=key-yLQxbJfOGIAEaTUWbWgc&amp;amp;show_recommendations=true&quot; data-auto-height=&quot;false&quot; data-aspect-ratio=&quot;0.7729220222793488&quot; scrolling=&quot;no&quot; id=&quot;doc_1544&quot; width=&quot;100%&quot; height=&quot;600&quot; frameborder=&quot;0&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;This was a joint work of –&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Bikramjot Hanzra (that’s me)&lt;/li&gt;
  &lt;li&gt;Tae-Hyung Kim&lt;/li&gt;
  &lt;li&gt;Rushat Gupta Chadha&lt;/li&gt;
  &lt;li&gt;Tushar Agrawal&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;–
@bikz05&lt;/p&gt;

</description>
        <pubDate>Fri, 05 Feb 2016 00:00:00 -0800</pubDate>
        <link>http://bikz05.github.io/2016/02/05/a-gentle-introduction-to-grasp-planning.html</link>
        <guid isPermaLink="true">http://bikz05.github.io/2016/02/05/a-gentle-introduction-to-grasp-planning.html</guid>
        
        <category>robotics</category>
        
        <category>manipulation</category>
        
        <category>planning</category>
        
        
        <category>robotics</category>
        
        <category>manipulation</category>
        
        <category>planning</category>
        
      </item>
    
      <item>
        <title>Caffe Installation on Ubuntu 14.04 (CPU) with PYTHON support</title>
        <description>&lt;p&gt;In this tutorial, I will detail the steps for installing Caffe on a non-GPU(CPU) machine with Ubuntu 14.04 OS. Once we are done with the installation, we’ll also test the &lt;a href=&quot;https://github.com/google/deepdream&quot;&gt;GOOGLE INCEPTIONISM (&lt;code&gt;deepdream&lt;/code&gt;) code&lt;/a&gt; which uses Caffe. This is a follow up post to my last post – &lt;a href=&quot;/2015/07/03/google-inceptionism.html&quot;&gt;Generating images with Google’s “INCEPTIONISM” – deepdream&lt;/a&gt; where I had shared some awesome images generated using &lt;code&gt;deepdream&lt;/code&gt; code. So, let’s get our hands dirty straight away &lt;img class=&quot;emoji&quot; title=&quot;:smile:&quot; alt=&quot;:smile:&quot; src=&quot;https://assets.github.com/images/icons/emoji/unicode/1f604.png&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; /&gt;.&lt;/p&gt;

&lt;p&gt;Firstly, let’s make a directory named &lt;code&gt;deep-learning&lt;/code&gt; in the &lt;code&gt;$HOME&lt;/code&gt; folder where we will download all the packages.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;nb&quot;&gt;cd&lt;/span&gt; ~
mkdir deep-learning
&lt;span class=&quot;nb&quot;&gt;cd &lt;/span&gt;deep-learning&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2 id=&quot;installing-the-dependencies&quot;&gt;1. Installing the dependencies&lt;/h2&gt;

&lt;p&gt;To successfully compile Caffe we need to install a few packages first. Execute the below command in the terminal. (&lt;em&gt;These commands will require root access&lt;/em&gt;)&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;NOTE -&amp;gt; Since we will not be using GPUs, we do not require CUDA installation.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;c&quot;&gt;# General Dependencies&lt;/span&gt;
sudo apt-get install libprotobuf-dev libleveldb-dev libsnappy-dev libopencv-dev libhdf5-serial-dev protobuf-compiler
sudo apt-get install --no-install-recommends libboost-all-dev

&lt;span class=&quot;c&quot;&gt;# BLAS -- for better CPU performance&lt;/span&gt;
sudo apt-get install libatlas-base-dev

&lt;span class=&quot;c&quot;&gt;# Python -- It comes preinstalled on Ubuntu 14.04&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Required if you want to use Python wrappers for Caffe&lt;/span&gt;
sudo apt-get install the python-dev

&lt;span class=&quot;c&quot;&gt;# Remaining dependencies&lt;/span&gt;
sudo apt-get install libgflags-dev libgoogle-glog-dev liblmdb-dev&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2 id=&quot;compile-and-install-caffe&quot;&gt;2. Compile and Install Caffe&lt;/h2&gt;

&lt;p&gt;The Caffe repository is hosted on GitHub. To clone the repository enter the below command in the terminal.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;nb&quot;&gt;cd&lt;/span&gt; ~/deep-learning
git clone http://github.com/BVLC/caffe.git
&lt;span class=&quot;nb&quot;&gt;cd &lt;/span&gt;caffe&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The &lt;code&gt;Makefile.config.example&lt;/code&gt; contains the template for configuration file. We’ll use it to write the &lt;code&gt;Makefile.config&lt;/code&gt; configuration file.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;cp Makefile.config.example Makefile.config&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In the &lt;code&gt;Makefile.config&lt;/code&gt; file we need to set options if we want to use a CPU or a GPU. Open your favorite editor and uncomment the line &lt;code&gt;CPU_ONLY := 1&lt;/code&gt;(most probably, line number 8) as shown below.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;lineno&quot;&gt;8&lt;/span&gt; CPU_ONLY :&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; 1&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;After setting the configuration, the next step is to make Caffe or brew Caffe &lt;img class=&quot;emoji&quot; title=&quot;:smile:&quot; alt=&quot;:smile:&quot; src=&quot;https://assets.github.com/images/icons/emoji/unicode/1f604.png&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; /&gt; .&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;make all
make &lt;span class=&quot;nb&quot;&gt;test&lt;/span&gt;
make runtest&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;note&quot;&gt;NOTE&lt;/h3&gt;

&lt;p&gt;If you get an OpenCV &lt;code&gt;undefined reference error&lt;/code&gt; in &lt;code&gt;make all&lt;/code&gt; as shown below follow the instructions below else move to the section 3.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;.build_release/lib/libcaffe.so: undefined reference to &lt;span class=&quot;sb&quot;&gt;`&lt;/span&gt;cv::imread&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;cv::String const&lt;span class=&quot;p&quot;&gt;&amp;amp;&lt;/span&gt;, int&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;&lt;/span&gt;
&lt;span class=&quot;s1&quot;&gt;.build_release/lib/libcaffe.so: undefined reference to `cv::imencode(cv::String const&amp;amp;, cv::_InputArray const&amp;amp;, std::vector&amp;lt;unsigned char, std::allocator&amp;lt;unsigned char&amp;gt; &amp;gt;&amp;amp;, std::vector&amp;lt;int, std::allocator&amp;lt;int&amp;gt; &amp;gt; const&amp;amp;)&amp;#39;&lt;/span&gt;
.build_release/lib/libcaffe.so: undefined reference to &lt;span class=&quot;sb&quot;&gt;`&lt;/span&gt;cv::imdecode&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;cv::_InputArray const&lt;span class=&quot;p&quot;&gt;&amp;amp;&lt;/span&gt;, int&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;err&quot;&gt;&amp;#39;&lt;/span&gt;
collect2: error: ld returned &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;exit &lt;/span&gt;status
make: *** &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;.build_release/tools/caffe.bin&lt;span class=&quot;o&quot;&gt;]&lt;/span&gt; Error 1&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;(Check &lt;a href=&quot;https://github.com/BVLC/caffe/issues/2348&quot;&gt;this&lt;/a&gt; issue) for more details. In order to fix the errror, append &lt;code&gt;opencv_imgcodecs&lt;/code&gt; on line 176(most probably) as shown below.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;lineno&quot;&gt;174&lt;/span&gt; LIBRARIES +&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; glog gflags protobuf leveldb snappy &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;175&lt;/span&gt; mdb boost_system hdf5_hl hdf5 m &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;176&lt;/span&gt; opencv_core opencv_highgui opencv_imgproc opencv_imgcodecs&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Now clear of the contents of &lt;code&gt;build&lt;/code&gt;. This is important else the error will persist.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;rm -rf ./build/*&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Now rerun the previous commands.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;make all
make &lt;span class=&quot;nb&quot;&gt;test&lt;/span&gt;
make runtest&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;adding-python-support&quot;&gt;3. Adding PYTHON support&lt;/h2&gt;

&lt;p&gt;To add PYTHON support, run the below commands in the terminal. We need Python support to run the &lt;code&gt;deepdream&lt;/code&gt; code.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;nb&quot;&gt;cd&lt;/span&gt; ~/deep-learning/caffe
make pycaffe
&lt;span class=&quot;nb&quot;&gt;echo export &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;PYTHONPATH&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;~/deep-learning/caffe/python:&lt;span class=&quot;nv&quot;&gt;$PYTHONPATH&lt;/span&gt; &amp;gt;&amp;gt; ~/.bashrc&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2 id=&quot;running-the-inceptionism-code&quot;&gt;4. Running the INCEPTIONISM code&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;p&gt;Read the INCEPTIONISM post by Google Research &lt;a href=&quot;http://googleresearch.blogspot.ch/2015/06/inceptionism-going-deeper-into-neural.html&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;To run the INCEPTIONISM code follow the below instructions.&lt;/p&gt;

&lt;h3 id=&quot;download-the-googlenet-model-and-clone-the-deepdream-github-repository&quot;&gt;4.1. Download the GoogLeNet model and clone the deepdream GitHub repository.&lt;/h3&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;nb&quot;&gt;cd&lt;/span&gt; ~/deep-learning
wget http://dl.caffe.berkeleyvision.org/bvlc_googlenet.caffemodel
mv bvlc_googlenet.caffemodel caffe/models/bvlc_googlenet/
git clone https://github.com/google/deepdream.git&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h3 id=&quot;install-the-dependencies&quot;&gt;4.2. Install the dependencies&lt;/h3&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;sudo pip install &lt;span class=&quot;s2&quot;&gt;&amp;quot;ipython[all]&amp;quot;&lt;/span&gt; 
sudo pip install numpy
sudo pip install scipy
sudo pip install protobuf
sudo pip install skimage&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h3 id=&quot;run-the-ipython-notebook&quot;&gt;4.3. Run the IPython Notebook&lt;/h3&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span class=&quot;nb&quot;&gt;cd&lt;/span&gt; ~/deep&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;learning/deepdream
ipython notebook&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Now the IPython notebook dashboard will open up in your default browser as shown below.&lt;/p&gt;

&lt;figure id=&quot;figure-1&quot;&gt;&lt;a href=&quot;/figures/2015-07-27-installing-caffe-on-ubuntu/ipython.png&quot;&gt;&lt;img src=&quot;/figures/2015-07-27-installing-caffe-on-ubuntu/ipython.png&quot; alt=&quot;Snapshot of IPython Notebook Dashboard&quot; /&gt;&lt;/a&gt;&lt;figcaption&gt;Figure 1: Snapshot of IPython Notebook Dashboard [&lt;a href=&quot;/figures/2015-07-27-installing-caffe-on-ubuntu/ipython.png&quot;&gt;PNG&lt;/a&gt;]&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;In the list of files, click on &lt;code&gt;dream.ipynb&lt;/code&gt; and you will get an output as shown in the figure below.&lt;/p&gt;

&lt;figure id=&quot;figure-2&quot;&gt;&lt;a href=&quot;/figures/2015-07-27-installing-caffe-on-ubuntu/inceptionism.png&quot;&gt;&lt;img src=&quot;/figures/2015-07-27-installing-caffe-on-ubuntu/inceptionism.png&quot; alt=&quot;Snapshot of IPython Notebook&quot; /&gt;&lt;/a&gt;&lt;figcaption&gt;Figure 2: Snapshot of IPython Notebook [&lt;a href=&quot;/figures/2015-07-27-installing-caffe-on-ubuntu/inceptionism.png&quot;&gt;PNG&lt;/a&gt;]&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;So, this is it ! Use this IPython Notebook to play with the &lt;code&gt;deepdream&lt;/code&gt; code to generate some awesome images. I hope you liked the post and THANK YOU &lt;img class=&quot;emoji&quot; title=&quot;:smile:&quot; alt=&quot;:smile:&quot; src=&quot;https://assets.github.com/images/icons/emoji/unicode/1f604.png&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; /&gt; for reading.&lt;/p&gt;

&lt;h2 id=&quot;important-links&quot;&gt;Important Links&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/BVLC/caffe&quot;&gt;Caffe GitHub repository&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/google/deepdream&quot;&gt;deepdream GitHub repository&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://dl.caffe.berkeleyvision.org/bvlc_googlenet.caffemodel&quot;&gt;GoogLeNet Caffe Model&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

</description>
        <pubDate>Mon, 27 Jul 2015 00:00:00 -0700</pubDate>
        <link>http://bikz05.github.io/2015/07/27/installing-caffe-on-ubuntu.html</link>
        <guid isPermaLink="true">http://bikz05.github.io/2015/07/27/installing-caffe-on-ubuntu.html</guid>
        
        <category>caffe</category>
        
        <category>google research</category>
        
        <category>ubuntu</category>
        
        <category>deep learning</category>
        
        
        <category>caffe</category>
        
        <category>google research</category>
        
        <category>ubuntu</category>
        
        <category>deep learning</category>
        
      </item>
    
      <item>
        <title>Generating images with Google&#39;s &quot;INCEPTIONISM&quot; -- deepdream</title>
        <description>&lt;p&gt;A couple of weeks ago Google Research blogged about a visualization technique which they named as “INCEPTIONISM” that could be used to make a pre-trained neural network generate intriguing picture. Alternatively, this technique can also be used to understand how well the neural network is learning at each layer. To get a high-level idea of what Google did, I recommend reading the original blog post &lt;a href=&quot;http://googleresearch.blogspot.in/2015/06/inceptionism-going-deeper-into-neural.html&quot;&gt;Inceptionism: Going Deeper into Neural Networks&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;This blog post got a lot of publicity in both the Computer Vision community and print media. So yesterday, Google announced in &lt;a href=&quot;DeepDream - a code example for visualizing Neural Networks&quot;&gt;this&lt;/a&gt; blog post that it had open sourced the code. The code is available on &lt;a href=&quot;https://github.com/google/deepdream&quot;&gt;GitHub&lt;/a&gt;. To clone the repository, enter the below command in the terminal.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;git clone https://github.com/google/deepdream.git&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In order to run the code, there are a few dependencies -&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Scientific Python Stack – &lt;code&gt;IPython Notebook&lt;/code&gt;, &lt;code&gt;numpy&lt;/code&gt;, &lt;code&gt;scipy&lt;/code&gt;, &lt;code&gt;matplotlib&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://caffe.berkeleyvision.org/&quot;&gt;Caffe&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;You will also need to download the GoogLeNet model.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;wget http://dl.caffe.berkeleyvision.org/bvlc_googlenet.caffemodel
mv bvlc_googlenet.caffemodel &amp;lt;path to Caffe installation&amp;gt;/caffe/models/bvlc_googlenet/&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Now let’s take a look at some of the mind blowing results!!&lt;/p&gt;

&lt;h3 id=&quot;nsa-headquarters&quot;&gt;NSA Headquarters&lt;/h3&gt;

&lt;center&gt;
&lt;blockquote class=&quot;twitter-tweet&quot; lang=&quot;en&quot;&gt;&lt;p lang=&quot;en&quot; dir=&quot;ltr&quot;&gt;&lt;a href=&quot;https://twitter.com/hashtag/deepdream?src=hash&quot;&gt;#deepdream&lt;/a&gt; NSA Headquarters. We all knew. &lt;a href=&quot;http://t.co/K7sTwERQCM&quot;&gt;pic.twitter.com/K7sTwERQCM&lt;/a&gt;&lt;/p&gt;&amp;mdash; samim (@samim) &lt;a href=&quot;https://twitter.com/samim/status/616642643083309056&quot;&gt;July 2, 2015&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async=&quot;&quot; src=&quot;//platform.twitter.com/widgets.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;
&lt;/center&gt;

&lt;h3 id=&quot;nebula&quot;&gt;NEBULA&lt;/h3&gt;

&lt;center&gt;
&lt;blockquote class=&quot;twitter-tweet&quot; lang=&quot;en&quot;&gt;&lt;p lang=&quot;en&quot; dir=&quot;ltr&quot;&gt;A rare photography of The Puppyslug Nebula from the Hubble Telescope.&amp;#10;&lt;a href=&quot;https://twitter.com/hashtag/deepdream?src=hash&quot;&gt;#deepdream&lt;/a&gt; &lt;a href=&quot;http://t.co/YjslsSlYAo&quot;&gt;pic.twitter.com/YjslsSlYAo&lt;/a&gt;&lt;/p&gt;&amp;mdash; Devine Lu Linvega (@aliceffekt) &lt;a href=&quot;https://twitter.com/aliceffekt/status/616632837274513408&quot;&gt;July 2, 2015&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async=&quot;&quot; src=&quot;//platform.twitter.com/widgets.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;
&lt;/center&gt;

&lt;p&gt;Since the neural network was trained mostly with images of animals, in most of the images you will see animals beings rendered in the images with dogs outnumbering any other creature on the planet. So, I decided to render crocodiles and was lucky to find some!!&lt;/p&gt;

&lt;h3 id=&quot;crocodiles&quot;&gt;CROCODILES&lt;/h3&gt;

&lt;center&gt;
&lt;blockquote class=&quot;twitter-tweet&quot; lang=&quot;en&quot;&gt;&lt;p lang=&quot;en&quot; dir=&quot;ltr&quot;&gt;Tired of seeing cats and dogs, here are crocodiles. &lt;a href=&quot;https://twitter.com/hashtag/deepdream?src=hash&quot;&gt;#deepdream&lt;/a&gt; &lt;a href=&quot;http://t.co/LUEwIQrvQc&quot;&gt;pic.twitter.com/LUEwIQrvQc&lt;/a&gt;&lt;/p&gt;&amp;mdash; Bikramjot S Hanzra (@bikz05) &lt;a href=&quot;https://twitter.com/bikz05/status/616700450214055936&quot;&gt;July 2, 2015&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async=&quot;&quot; src=&quot;//platform.twitter.com/widgets.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;
&lt;/center&gt;

&lt;h3 id=&quot;taj-mahal-inceptionism&quot;&gt;TAJ MAHAL INCEPTIONISM&lt;/h3&gt;

&lt;p&gt;I also created a ~ 66 frame TAJ MAHAL INCEPTIONISM video (20fps).&lt;/p&gt;

&lt;center&gt;
&lt;blockquote class=&quot;twitter-tweet&quot; lang=&quot;en&quot;&gt;&lt;p lang=&quot;tr&quot; dir=&quot;ltr&quot;&gt;Taj Mahal &lt;a href=&quot;https://twitter.com/hashtag/INCEPTIONISM?src=hash&quot;&gt;#INCEPTIONISM&lt;/a&gt; &lt;a href=&quot;https://twitter.com/hashtag/deepdream?src=hash&quot;&gt;#deepdream&lt;/a&gt; &lt;a href=&quot;https://twitter.com/hashtag/deeplearning?src=hash&quot;&gt;#deeplearning&lt;/a&gt; &lt;a href=&quot;https://t.co/7l1FjUPoR6&quot;&gt;https://t.co/7l1FjUPoR6&lt;/a&gt; via &lt;a href=&quot;https://twitter.com/YouTube&quot;&gt;@YouTube&lt;/a&gt;&lt;/p&gt;&amp;mdash; Bikramjot S Hanzra (@bikz05) &lt;a href=&quot;https://twitter.com/bikz05/status/616967565152104448&quot;&gt;July 3, 2015&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async=&quot;&quot; src=&quot;//platform.twitter.com/widgets.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;
&lt;/center&gt;

&lt;p&gt;Here are some of the frames from the video.&lt;/p&gt;

&lt;figure id=&quot;figure-1&quot;&gt;&lt;a href=&quot;/figures/2015-07-03-google-inceptionism/0005.jpg&quot;&gt;&lt;img src=&quot;/figures/2015-07-03-google-inceptionism/0005.jpg&quot; alt=&quot;Frame 5 -- TAJ MAHAL INCEPTIONISM&quot; /&gt;&lt;/a&gt;&lt;figcaption&gt;Figure 1: Frame 5 -- TAJ MAHAL INCEPTIONISM [&lt;a href=&quot;/figures/2015-07-03-google-inceptionism/0005.jpg&quot;&gt;JPG&lt;/a&gt;]&lt;/figcaption&gt;&lt;/figure&gt;

&lt;figure id=&quot;figure-2&quot;&gt;&lt;a href=&quot;/figures/2015-07-03-google-inceptionism/0018.jpg&quot;&gt;&lt;img src=&quot;/figures/2015-07-03-google-inceptionism/0018.jpg&quot; alt=&quot;Frame 18 -- TAJ MAHAL INCEPTIONISM&quot; /&gt;&lt;/a&gt;&lt;figcaption&gt;Figure 2: Frame 18 -- TAJ MAHAL INCEPTIONISM [&lt;a href=&quot;/figures/2015-07-03-google-inceptionism/0018.jpg&quot;&gt;JPG&lt;/a&gt;]&lt;/figcaption&gt;&lt;/figure&gt;

&lt;figure id=&quot;figure-3&quot;&gt;&lt;a href=&quot;/figures/2015-07-03-google-inceptionism/0030.jpg&quot;&gt;&lt;img src=&quot;/figures/2015-07-03-google-inceptionism/0030.jpg&quot; alt=&quot;Frame 30 -- TAJ MAHAL INCEPTIONISM&quot; /&gt;&lt;/a&gt;&lt;figcaption&gt;Figure 3: Frame 30 -- TAJ MAHAL INCEPTIONISM [&lt;a href=&quot;/figures/2015-07-03-google-inceptionism/0030.jpg&quot;&gt;JPG&lt;/a&gt;]&lt;/figcaption&gt;&lt;/figure&gt;

&lt;figure id=&quot;figure-4&quot;&gt;&lt;a href=&quot;/figures/2015-07-03-google-inceptionism/0056.jpg&quot;&gt;&lt;img src=&quot;/figures/2015-07-03-google-inceptionism/0056.jpg&quot; alt=&quot;Frame 56 -- TAJ MAHAL INCEPTIONISM&quot; /&gt;&lt;/a&gt;&lt;figcaption&gt;Figure 4: Frame 56 -- TAJ MAHAL INCEPTIONISM [&lt;a href=&quot;/figures/2015-07-03-google-inceptionism/0056.jpg&quot;&gt;JPG&lt;/a&gt;]&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;To discover more, use the hashtag &lt;code&gt;#deepdream&lt;/code&gt; on Twitter.&lt;/p&gt;

&lt;p&gt;If this wasn’t enough some researchers even released a few projects related to “INCEPTIONISM”. Here are a couple of them.&lt;/p&gt;

&lt;h3 id=&quot;dockerized-deepdream-generate-convnet-art-in-the-cloudhttpsgithubcomvisionaiclouddream&quot;&gt;&lt;a href=&quot;https://github.com/VISIONAI/clouddream&quot;&gt;Dockerized deepdream: Generate ConvNet Art in the Cloud&lt;/a&gt;&lt;/h3&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;git clone https://github.com/VISIONAI/clouddream.git&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;Let’s make it brain-dead simple to launch your very own deepdreaming server (in the cloud, on an Ubuntu machine, Mac via Docker, and maybe even Windows if you try out Kitematic by Docker)!&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;cnn-vishttpsgithubcomjcjohnsoncnn-vis&quot;&gt;&lt;a href=&quot;https://github.com/jcjohnson/cnn-vis&quot;&gt;cnn-vis&lt;/a&gt;&lt;/h3&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;git clone https://github.com/jcjohnson/cnn-vis.git&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;Inspired by Google’s recent Inceptionism blog post, cnn-vis is an open-source tool that lets you use convolutional neural networks to generate images.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;update&quot;&gt;Update&lt;/h3&gt;

&lt;p&gt;Check out my friend Adrian’s &lt;a href=&quot;https://github.com/jrosebr1/bat-country&quot;&gt;bat-county&lt;/a&gt; repository on GitHub.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;A lightweight, extendible, easy to use Python package for deep dreaming and image generation with Caffe and CNNs.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;hr /&gt;

&lt;p&gt;A lot of people wanted to try the &lt;code&gt;deepdream&lt;/code&gt; code, but were unable to setup the environment. As a follow-up, I will soon blog about setting up Caffe on Ubuntu 14.04(CPU) and running the &lt;code&gt;deepdream&lt;/code&gt; code.&lt;/p&gt;

&lt;h3 id=&quot;update---check-out-my-new-post-on-caffe-installation-on-ubuntu-1404-cpu20150703installing-caffe-on-ubuntuhtml&quot;&gt;UPDATE - Check out my new post on &lt;a href=&quot;/2015/07/03/installing-caffe-on-ubuntu.html&quot;&gt;Caffe Installation on Ubuntu 14.04 (CPU)&lt;/a&gt;.&lt;/h3&gt;
</description>
        <pubDate>Fri, 03 Jul 2015 00:00:00 -0700</pubDate>
        <link>http://bikz05.github.io/2015/07/03/google-inceptionism.html</link>
        <guid isPermaLink="true">http://bikz05.github.io/2015/07/03/google-inceptionism.html</guid>
        
        <category>ipython</category>
        
        <category>google research</category>
        
        <category>neural networks</category>
        
        <category>deep learning</category>
        
        
        <category>ipython</category>
        
        <category>google research</category>
        
        <category>neural networks</category>
        
        <category>deep learning</category>
        
      </item>
    
      <item>
        <title>Texture Matching using Local Binary Patterns (LBP), OpenCV, scikit-learn and Python</title>
        <description>&lt;style&gt;.embed-container { position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; } .embed-container iframe, .embed-container object, .embed-container embed { position: absolute; top: 0; left: 0; width: 100%; height: 100%; }&lt;/style&gt;
&lt;div class=&quot;embed-container&quot;&gt;&lt;iframe src=&quot;http://www.youtube.com/embed/lIgqjRe88_s&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;In this tutorial, I will discuss about how to perform texture matching using Local Binary Patterns (LBP). Local Binary Patterns is an important feature descriptor that is used in computer vision for texture matching. It was first released in 1990 and subsequently various modified versions have been released.&lt;/p&gt;

&lt;h1 id=&quot;lbp-descriptor&quot;&gt;LBP Descriptor&lt;/h1&gt;

&lt;p&gt;Let’s first discuss how to calculate the LBP Descriptor. Firstly, we convert the input color image to grayscale, since LBP works on grayscale images. For each pixel in the grayscale image, a neighbourhood is selected around the current pixel and then we calculate the LBP value for the pixel using the neighbourhood. After calculating the LBP value of the current pixel, we update the corresponding pixel location in the LBP mask (It is of same height and width as the input image.) with the LBP value calculated as shown below. In the image, we have 8 neighbouring pixels.&lt;/p&gt;
&lt;figure id=&quot;figure-1&quot;&gt;&lt;a href=&quot;/figures/2015-05-30-local-binary-patterns/lbp-2.png&quot;&gt;&lt;img src=&quot;/figures/2015-05-30-local-binary-patterns/lbp-2.png&quot; alt=&quot;Grayscale Image to LBP Mask *&quot; /&gt;&lt;/a&gt;&lt;figcaption&gt;Figure 1: Grayscale Image to LBP Mask * [&lt;a href=&quot;/figures/2015-05-30-local-binary-patterns/lbp-2.png&quot;&gt;PNG&lt;/a&gt;]&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;&lt;em&gt;But how is the LBP values calculated?&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;To calculate the LBP value for a pixel in the grayscale image, we compare the central pixel value with the neighbouring pixel values. We can start from any neighbouring pixel and then we can transverse either in clockwise or anti-clockwise direction but we must use the same order for all the pixels. Since there are 8 neighbouring pixels – for each pixel, we will perform 8 comparisons. The results of the comparisons are stored in a 8-bit binary array.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;If the current pixel value is greater or equal to the neighbouring pixel value, the corresponding bit in the binary array is set to 1 else if the current pixel value is less than the neighbouring pixel value, the corresponding bit in the binary array is set to 0.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The whole process is shown in the image below (Figure 2). The current (central) pixel has value 7. We start comparing from the neighbouring pixel where the label 0. The value of the neighbouring pixel with label 0 is 2. Since it is less than the current pixel value which is 7, we reset the 0th bit location in the 8 bit binary array to 0. We then iterate in the counter-clockwise direction. The next label location 1 have value 7 which is equal to the current pixel value, so we set the 1st bit location in the 8 bit binary to 1. We then continue to move to the next neighbouring pixel until we reach the 8th neighbouring pixel. Then the 8-bit binary pattern is converted to a decimal number and the decimal number is then stored in the corresponding pixel location in the LBP mask.&lt;/p&gt;

&lt;figure id=&quot;figure-2&quot;&gt;&lt;a href=&quot;/figures/2015-05-30-local-binary-patterns/lbp-1.png&quot;&gt;&lt;img src=&quot;/figures/2015-05-30-local-binary-patterns/lbp-1.png&quot; alt=&quot;Calculation of LBP values *&quot; /&gt;&lt;/a&gt;&lt;figcaption&gt;Figure 2: Calculation of LBP values * [&lt;a href=&quot;/figures/2015-05-30-local-binary-patterns/lbp-1.png&quot;&gt;PNG&lt;/a&gt;]&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;Once we have calculated the LBP Mask, we calculate the LBP histogram. The LBP mask values range from 0 to 255, so our LBP Descriptor will be of size 1x256. We then normalize the LBP histogram. The image below shows the scheme of the algorithm -&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Load the color image.&lt;/li&gt;
  &lt;li&gt;Convert to grayscale image.&lt;/li&gt;
  &lt;li&gt;Calculate the LBP mask.&lt;/li&gt;
  &lt;li&gt;Calculate the LBP Histogram and normalize it.&lt;/li&gt;
&lt;/ol&gt;

&lt;figure id=&quot;figure-3&quot;&gt;&lt;a href=&quot;/figures/2015-05-30-local-binary-patterns/lbp-3.png&quot;&gt;&lt;img src=&quot;/figures/2015-05-30-local-binary-patterns/lbp-3.png&quot; alt=&quot;Color Image -&amp;gt; Grayscale Image -&amp;gt; LBP Mask -&amp;gt; Normalized LBP Histogram *&quot; /&gt;&lt;/a&gt;&lt;figcaption&gt;Figure 3: Color Image -&amp;gt; Grayscale Image -&amp;gt; LBP Mask -&amp;gt; Normalized LBP Histogram * [&lt;a href=&quot;/figures/2015-05-30-local-binary-patterns/lbp-3.png&quot;&gt;PNG&lt;/a&gt;]&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;One advantage of LBP is that it is illumination and translation invariant. We have selected a 8 point neighbourhood, but most implementations use a circular neighbourhood as shown below. In the code, we will use a circular neighbourhood.&lt;/p&gt;

&lt;figure id=&quot;figure-4&quot;&gt;&lt;a href=&quot;/figures/2015-05-30-local-binary-patterns/lbp-4.png&quot;&gt;&lt;img src=&quot;/figures/2015-05-30-local-binary-patterns/lbp-4.png&quot; alt=&quot;Circular Neighbourhood *&quot; /&gt;&lt;/a&gt;&lt;figcaption&gt;Figure 4: Circular Neighbourhood * [&lt;a href=&quot;/figures/2015-05-30-local-binary-patterns/lbp-4.png&quot;&gt;PNG&lt;/a&gt;]&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;The original LBP algorithm has been further optimised to give better results. One such implementation is the &lt;strong&gt;Uniform LBP&lt;/strong&gt;. In the code, we will use Uniform LBP.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;That’s LBP for you&lt;/em&gt;, now let’s get started with the programming part &lt;img class=&quot;emoji&quot; title=&quot;:smile:&quot; alt=&quot;:smile:&quot; src=&quot;https://assets.github.com/images/icons/emoji/unicode/1f604.png&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;* Image taken from &lt;a href=&quot;https://class.coursera.org/deteccionobjetos-001/&quot;&gt;Detección de objetos&lt;/a&gt; course by Antonio López Peña, Ernest Valveny, Maria Vanrell (UAB) on Coursera.&lt;/em&gt;&lt;/p&gt;

&lt;h1 id=&quot;import-the-required-modules&quot;&gt;Import the required modules&lt;/h1&gt;

&lt;p&gt;The first step is to import the required modules. Since I am working in IPython Notebook, I have imported the &lt;code&gt;pylab&lt;/code&gt; module in inline mode since it seamlessly embeds the images within the notebook. Otherwise, a seperate window opens which makes working with IPython Notebooks cumbersome. We will use the &lt;code&gt;cv2&lt;/code&gt; module to read the images, perform color space transformations etc. The &lt;code&gt;os&lt;/code&gt; module is used to perform path manipulations. We will leverage the &lt;code&gt;local_binary_pattern&lt;/code&gt; function from the &lt;code&gt;skimage.feature&lt;/code&gt; module to calculate the LBP mask. From the LBP mask we will calculate the LBP histogram using &lt;code&gt;scipy.stats.itemfreq&lt;/code&gt; function and then we will use the &lt;code&gt;sklearn.preprocessing.normalize&lt;/code&gt; function to normalize the histogram. &lt;code&gt;cvutils&lt;/code&gt; is a utility package for working with computer vision and image processing packages. Use the command, &lt;code&gt;pip install cvutils&lt;/code&gt; to install the package. Finally, the &lt;code&gt;csv&lt;/code&gt; module provides functionality to parse text files.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pylab&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inline&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;no&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;import&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;all&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# OpenCV bindings&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;cv2&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# To performing path manipulations &lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;os&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Local Binary Pattern function&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;skimage.feature&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;local_binary_pattern&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# To calculate a normalized histogram &lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;scipy.stats&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;itemfreq&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.preprocessing&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;normalize&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Utility package -- use pip install cvutils to install&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;cvutils&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# To read class from file&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;csv&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h1 id=&quot;prepare-the-training-set&quot;&gt;Prepare the training set&lt;/h1&gt;
&lt;p&gt;I have assorted 6 training images. The 6 images consists of 2 images of rocks, 2 images of grass and 2 images of checkered patterns. I have also created a &lt;code&gt;class_train.txt&lt;/code&gt; file which looks like -&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;rock-1.jpg 0
rock-2.jpg 0
grass-1.jpg 1
grass-2.jpg 1
checkered-1.jpg 2
checkered-2.jpg 2&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Each line consists of the image name followed by the class label. I have given the following labels for each class -&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Class&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Label&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Rock&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Grass&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;Checkered&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Although, I haven’t used the class labels in this tutorial but it is always better to prepare a structure that can be used later on. For example, these labels can be useful if let’s say an SVM is used for classification.&lt;/p&gt;

&lt;p&gt;So, let’s write the code to store the path of all the images in the training set.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;lineno&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# Store the path of training images in train_images&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_images&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cvutils&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imlist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;../data/lbp/train/&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;3&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# Dictionary containing image paths as keys and corresponding label as value&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;4&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_dic&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{}&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;5&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;open&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;#39;../data/lbp/class_train.txt&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;#39;rb&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;csvfile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;6&lt;/span&gt;     &lt;span class=&quot;n&quot;&gt;reader&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;csv&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;csvfile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;delimiter&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;#39; &amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;7&lt;/span&gt;     &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;row&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;8&lt;/span&gt;         &lt;span class=&quot;n&quot;&gt;train_dic&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;row&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;row&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In line 2, we store all the paths of the training images in a list named &lt;code&gt;train_images&lt;/code&gt;. In line 4, we create a dictionary &lt;code&gt;train_dic&lt;/code&gt; that will contain the image name and the corresponding class label. From line 5 to 8, we read lines from the &lt;code&gt;class_train.txt&lt;/code&gt; document described above and  then we add the key-value pair – (image name, class label) to &lt;code&gt;train-dic&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;&lt;code&gt;train_images&lt;/code&gt; contains the following image paths -&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;#39;../data/lbp/train/rock-1.jpg&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;#39;../data/lbp/train/rock-2.jpg&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;#39;../data/lbp/train/grass-2.jpg&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;#39;../data/lbp/train/checkered-2.jpg&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;#39;../data/lbp/train/checkered-1.jpg&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;#39;../data/lbp/train/grass-1.jpg&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;code&gt;train-dic&lt;/code&gt; contains the following key-value pairs -&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;#39;grass-1.jpg&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;#39;rock-1.jpg&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;#39;checkered-2.jpg&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;#39;rock-2.jpg&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;#39;checkered-1.jpg&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;#39;grass-2.jpg&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h1 id=&quot;calculate-the-lbp-histograms&quot;&gt;Calculate the LBP Histograms&lt;/h1&gt;

&lt;p&gt;Now the next step is to calculate the normalized LBP histograms of the training images. Here is the code for the same -&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;lineno&quot;&gt; 1&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# List for storing the LBP Histograms, address of images and the corresponding label &lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt; 2&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_test&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt; 3&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_name&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt; 4&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_test&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt; 5&lt;/span&gt; 
&lt;span class=&quot;lineno&quot;&gt; 6&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# For each image in the training set calculate the LBP histogram&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt; 7&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# and update X_test, X_name and y_test&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt; 8&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_image&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_images&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt; 9&lt;/span&gt;     &lt;span class=&quot;c&quot;&gt;# Read the image&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;10&lt;/span&gt;     &lt;span class=&quot;n&quot;&gt;im&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imread&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;11&lt;/span&gt;     &lt;span class=&quot;c&quot;&gt;# Convert to grayscale as LBP works on grayscale image&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;12&lt;/span&gt;     &lt;span class=&quot;n&quot;&gt;im_gray&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cvtColor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;im&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;COLOR_BGR2GRAY&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;13&lt;/span&gt;     &lt;span class=&quot;n&quot;&gt;radius&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;14&lt;/span&gt;     &lt;span class=&quot;c&quot;&gt;# Number of points to be considered as neighbourers &lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;15&lt;/span&gt;     &lt;span class=&quot;n&quot;&gt;no_points&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;radius&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;16&lt;/span&gt;     &lt;span class=&quot;c&quot;&gt;# Uniform LBP is used&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;17&lt;/span&gt;     &lt;span class=&quot;n&quot;&gt;lbp&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;local_binary_pattern&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;im_gray&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;no_points&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;radius&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;method&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;#39;uniform&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;18&lt;/span&gt;     &lt;span class=&quot;c&quot;&gt;# Calculate the histogram&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;19&lt;/span&gt;     &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;itemfreq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lbp&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ravel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;20&lt;/span&gt;     &lt;span class=&quot;c&quot;&gt;# Normalize the histogram&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;21&lt;/span&gt;     &lt;span class=&quot;n&quot;&gt;hist&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;22&lt;/span&gt;     &lt;span class=&quot;c&quot;&gt;# Append image path in X_name&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;23&lt;/span&gt;     &lt;span class=&quot;n&quot;&gt;X_name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;24&lt;/span&gt;     &lt;span class=&quot;c&quot;&gt;# Append histogram to X_name&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;25&lt;/span&gt;     &lt;span class=&quot;n&quot;&gt;X_test&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;26&lt;/span&gt;     &lt;span class=&quot;c&quot;&gt;# Append class label in y_test&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;27&lt;/span&gt;     &lt;span class=&quot;n&quot;&gt;y_test&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_dic&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;28&lt;/span&gt;     
&lt;span class=&quot;lineno&quot;&gt;29&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# Display the training images&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;30&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nrows&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;31&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ncols&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;32&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axes&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;subplots&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nrows&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ncols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;33&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;row&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nrows&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;34&lt;/span&gt;     &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;col&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ncols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;35&lt;/span&gt;         &lt;span class=&quot;n&quot;&gt;axes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;row&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;col&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imshow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cvtColor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imread&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;row&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ncols&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;col&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;COLOR_BGR2RGB&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;36&lt;/span&gt;         &lt;span class=&quot;n&quot;&gt;axes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;row&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;col&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;#39;off&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;37&lt;/span&gt;         &lt;span class=&quot;n&quot;&gt;axes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;row&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;col&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;{}&amp;quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;row&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ncols&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;col&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]))&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Let’s break up the above code. In line 2-4, we create 3 lists –&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;code&gt;X_test&lt;/code&gt; - to store the normalized LBP Histograms&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;X_name&lt;/code&gt; - to store the address of images&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;y_test&lt;/code&gt; - to store the corresponding class label&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Each index in all the 3 lists corresponds to the same image.&lt;/p&gt;

&lt;p&gt;From line 8 to 27 we loop over all the images in the training set and calculate the normalized LBP histograms for the training images. So firstly in line 10, we read the current image using the &lt;code&gt;cv2.imread&lt;/code&gt; function. We then convert the image to grayscale since LBP works on grayscale image. In line 17, we calculate the LBP mask. We set the radius of the neighbourhood to 3 and the number of points to be equal to 24. Once we have the mask, we calculate the LBP histogram in line 19 and normalize it in line 21.
Next, we append the histogram to the list &lt;code&gt;X_test&lt;/code&gt;in line 25. We also append the image name to the list &lt;code&gt;X_name&lt;/code&gt; and the image class label to &lt;code&gt;y_test&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Next from line 30 to 37, we display the 6 training images. The image generated is -&lt;/p&gt;

&lt;figure id=&quot;figure-5&quot;&gt;&lt;a href=&quot;/figures/2015-05-30-local-binary-patterns/training-set.png&quot;&gt;&lt;img src=&quot;/figures/2015-05-30-local-binary-patterns/training-set.png&quot; alt=&quot;Testing Set&quot; /&gt;&lt;/a&gt;&lt;figcaption&gt;Figure 5: Testing Set [&lt;a href=&quot;/figures/2015-05-30-local-binary-patterns/training-set.png&quot;&gt;PNG&lt;/a&gt;]&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h1 id=&quot;get-the-testing-images&quot;&gt;Get the testing images&lt;/h1&gt;

&lt;p&gt;To test the performance of the LBP algorithm, I have again assorted 3 images of each class not present in the training set. We will read the paths of the 3 images and append them to a list named &lt;code&gt;test_images&lt;/code&gt;. We will also create a dictionary &lt;code&gt;test_dic&lt;/code&gt; similar to the dictionary &lt;code&gt;train_dic&lt;/code&gt; created above. Here is the code for the same-&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;lineno&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# Store the path of testing images in test_images&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;test_images&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cvutils&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imlist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;../data/lbp/test/&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;3&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# Dictionary containing image paths as keys and corresponding label as value&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;4&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;test_dic&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{}&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;5&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;open&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;#39;../data/lbp/class_test.txt&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;#39;rb&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;csvfile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;6&lt;/span&gt;     &lt;span class=&quot;n&quot;&gt;reader&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;csv&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;csvfile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;delimiter&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;#39; &amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;7&lt;/span&gt;     &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;row&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;8&lt;/span&gt;         &lt;span class=&quot;n&quot;&gt;test_dic&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;row&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;row&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The contents of &lt;code&gt;test_images&lt;/code&gt; and &lt;code&gt;test_dic&lt;/code&gt; are -&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;c&quot;&gt;# test_images&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;#39;../data/lbp/test/rock-1.png&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;#39;../data/lbp/test/checkered-1.jpg&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;#39;../data/lbp/test/grass-1.jpg&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# test_dic&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;#39;rock-1.png&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;#39;grass-1.jpg&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;#39;checkered-1.jpg&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h1 id=&quot;calculate-the-chi-squared-distance-for-each-testing-image&quot;&gt;Calculate the Chi-squared distance for each testing image&lt;/h1&gt;

&lt;p&gt;We then calculate the normalized LBP histograms for each image in the testing set and then we compare the normalized LBP Histograms of the training images (calculated above) with these using the Chi-Squared distance metric. We then sort the results based on the Chi-Squared distance and display the results in sorted order. &lt;strong&gt;The lower the Chi-Squared distance, the better is the match.&lt;/strong&gt;&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;lineno&quot;&gt; 1&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;test_image&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;test_images&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt; 2&lt;/span&gt;      &lt;span class=&quot;c&quot;&gt;# Read the image&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt; 3&lt;/span&gt;     &lt;span class=&quot;n&quot;&gt;im&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imread&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;test_image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt; 4&lt;/span&gt;     &lt;span class=&quot;c&quot;&gt;# Convert to grayscale as LBP works on grayscale image&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt; 5&lt;/span&gt;     &lt;span class=&quot;n&quot;&gt;im_gray&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cvtColor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;im&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;COLOR_BGR2GRAY&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt; 6&lt;/span&gt;     &lt;span class=&quot;n&quot;&gt;radius&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt; 7&lt;/span&gt;     &lt;span class=&quot;c&quot;&gt;# Number of points to be considered as neighbourers &lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt; 8&lt;/span&gt;     &lt;span class=&quot;n&quot;&gt;no_points&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;radius&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt; 9&lt;/span&gt;     &lt;span class=&quot;c&quot;&gt;# Uniform LBP is used&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;10&lt;/span&gt;     &lt;span class=&quot;n&quot;&gt;lbp&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;local_binary_pattern&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;im_gray&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;no_points&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;radius&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;method&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;#39;uniform&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;11&lt;/span&gt;     &lt;span class=&quot;c&quot;&gt;# Calculate the histogram&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;12&lt;/span&gt;     &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;itemfreq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lbp&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ravel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;13&lt;/span&gt;     &lt;span class=&quot;c&quot;&gt;# Normalize the histogram&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;14&lt;/span&gt;     &lt;span class=&quot;n&quot;&gt;hist&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;15&lt;/span&gt;     &lt;span class=&quot;c&quot;&gt;# Display the query image&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;16&lt;/span&gt;     &lt;span class=&quot;n&quot;&gt;cvutils&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imshow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;** Query Image -&amp;gt; {}**&amp;quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;test_image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;im&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;17&lt;/span&gt;     &lt;span class=&quot;n&quot;&gt;results&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;18&lt;/span&gt;     &lt;span class=&quot;c&quot;&gt;# For each image in the training dataset&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;19&lt;/span&gt;     &lt;span class=&quot;c&quot;&gt;# Calculate the chi-squared distance and the sort the values&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;20&lt;/span&gt;     &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;index&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;enumerate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;21&lt;/span&gt;         &lt;span class=&quot;n&quot;&gt;score&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;compareHist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;float32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cv&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;CV_COMP_CHISQR&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;22&lt;/span&gt;         &lt;span class=&quot;n&quot;&gt;results&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X_name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;index&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;23&lt;/span&gt;     &lt;span class=&quot;n&quot;&gt;results&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;sorted&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;results&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;lambda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;24&lt;/span&gt;     &lt;span class=&quot;c&quot;&gt;# Display the results&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;25&lt;/span&gt;     &lt;span class=&quot;n&quot;&gt;nrows&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;26&lt;/span&gt;     &lt;span class=&quot;n&quot;&gt;ncols&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;27&lt;/span&gt;     &lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axes&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;subplots&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nrows&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ncols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;28&lt;/span&gt;     &lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;suptitle&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;** Scores for -&amp;gt; {}**&amp;quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;test_image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;29&lt;/span&gt;     &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;row&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nrows&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;30&lt;/span&gt;         &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;col&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ncols&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;31&lt;/span&gt;             &lt;span class=&quot;n&quot;&gt;axes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;row&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;col&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imshow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cvtColor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imread&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;results&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;row&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ncols&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;col&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;COLOR_BGR2RGB&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;32&lt;/span&gt;             &lt;span class=&quot;n&quot;&gt;axes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;row&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;col&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;#39;off&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;33&lt;/span&gt;             &lt;span class=&quot;n&quot;&gt;axes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;row&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;col&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;Score {}&amp;quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;results&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;row&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ncols&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;col&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]))&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;From line 1 to 14, we calculate the normalized LBP histograms as in the case of training images. In line 21, we calculate the Chi-Squared Distance of the testing image with all the training images using the &lt;code&gt;cv2.compareHist&lt;/code&gt; function. Then, we sort the scores in line 22. From line 25 to 33, we display the training images with the corresponding score.&lt;/p&gt;

&lt;h1 id=&quot;results&quot;&gt;Results&lt;/h1&gt;

&lt;p&gt;Let’s check out the results.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Input Image – ROCK Class&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Here is an input image of a rock texture.&lt;/p&gt;
&lt;figure id=&quot;figure-6&quot;&gt;&lt;a href=&quot;/figures/2015-05-30-local-binary-patterns/query-image-1.png&quot;&gt;&lt;img src=&quot;/figures/2015-05-30-local-binary-patterns/query-image-1.png&quot; alt=&quot;Query Image of Rock Texture&quot; /&gt;&lt;/a&gt;&lt;figcaption&gt;Figure 6: Query Image of Rock Texture [&lt;a href=&quot;/figures/2015-05-30-local-binary-patterns/query-image-1.png&quot;&gt;PNG&lt;/a&gt;]&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;&lt;em&gt;Matching Scores&lt;/em&gt; - Below are the sorted results of matching. The top 2 scores are of rock texture.&lt;/p&gt;
&lt;figure id=&quot;figure-7&quot;&gt;&lt;a href=&quot;/figures/2015-05-30-local-binary-patterns/query-image-results-1.png&quot;&gt;&lt;img src=&quot;/figures/2015-05-30-local-binary-patterns/query-image-results-1.png&quot; alt=&quot;Matching Scores&quot; /&gt;&lt;/a&gt;&lt;figcaption&gt;Figure 7: Matching Scores [&lt;a href=&quot;/figures/2015-05-30-local-binary-patterns/query-image-results-1.png&quot;&gt;PNG&lt;/a&gt;]&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;Input Image – CHECKERED Class&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Here is an input image of a checkered texture .&lt;/p&gt;
&lt;figure id=&quot;figure-8&quot;&gt;&lt;a href=&quot;/figures/2015-05-30-local-binary-patterns/query-image-2.png&quot;&gt;&lt;img src=&quot;/figures/2015-05-30-local-binary-patterns/query-image-2.png&quot; alt=&quot;Query Image of Checkered Texture&quot; /&gt;&lt;/a&gt;&lt;figcaption&gt;Figure 8: Query Image of Checkered Texture [&lt;a href=&quot;/figures/2015-05-30-local-binary-patterns/query-image-2.png&quot;&gt;PNG&lt;/a&gt;]&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;&lt;em&gt;Matching Scores&lt;/em&gt; - Below are the sorted results of matching. The top 2 scores are of checkered texture.&lt;/p&gt;
&lt;figure id=&quot;figure-9&quot;&gt;&lt;a href=&quot;/figures/2015-05-30-local-binary-patterns/query-image-results-2.png&quot;&gt;&lt;img src=&quot;/figures/2015-05-30-local-binary-patterns/query-image-results-2.png&quot; alt=&quot;Matching Scores&quot; /&gt;&lt;/a&gt;&lt;figcaption&gt;Figure 9: Matching Scores [&lt;a href=&quot;/figures/2015-05-30-local-binary-patterns/query-image-results-2.png&quot;&gt;PNG&lt;/a&gt;]&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;Input Image – GRASS Class&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Here is an input image of a grass texture.&lt;/p&gt;
&lt;figure id=&quot;figure-10&quot;&gt;&lt;a href=&quot;/figures/2015-05-30-local-binary-patterns/query-image-3.png&quot;&gt;&lt;img src=&quot;/figures/2015-05-30-local-binary-patterns/query-image-3.png&quot; alt=&quot;Query Image of Grass Texture&quot; /&gt;&lt;/a&gt;&lt;figcaption&gt;Figure 10: Query Image of Grass Texture [&lt;a href=&quot;/figures/2015-05-30-local-binary-patterns/query-image-3.png&quot;&gt;PNG&lt;/a&gt;]&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;&lt;em&gt;Matching Scores&lt;/em&gt; - Below are the sorted results of matching. The top 2 scores are of grass texture.&lt;/p&gt;
&lt;figure id=&quot;figure-11&quot;&gt;&lt;a href=&quot;/figures/2015-05-30-local-binary-patterns/query-image-results-3.png&quot;&gt;&lt;img src=&quot;/figures/2015-05-30-local-binary-patterns/query-image-results-3.png&quot; alt=&quot;Matching Scores&quot; /&gt;&lt;/a&gt;&lt;figcaption&gt;Figure 11: Matching Scores [&lt;a href=&quot;/figures/2015-05-30-local-binary-patterns/query-image-results-3.png&quot;&gt;PNG&lt;/a&gt;]&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;Inference – In each case, the best 2 results outperform the next matches by atleast a single significant digit – sufficient to prove the potency of LBP for texture matching.&lt;/strong&gt;&lt;/p&gt;

&lt;h1 id=&quot;download-the-code&quot;&gt;Download the code&lt;/h1&gt;

&lt;p&gt;&lt;span class=&quot;bikzclick&quot;&gt;
&lt;a href=&quot;http://bit.ly/1bWfPBF&quot;&gt;Click here to Download the code&lt;/a&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;img class=&quot;emoji&quot; title=&quot;:octocat:&quot; alt=&quot;:octocat:&quot; src=&quot;https://assets.github.com/images/icons/emoji/octocat.png&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; /&gt; View the GitHub repository &lt;a href=&quot;https://github.com/bikz05/texture-matching&quot;&gt;here&lt;/a&gt;. &lt;!-- Place this tag where you want the button to render. --&gt;&lt;/li&gt;
  &lt;li&gt;Download the Ipython Notebook from &lt;a href=&quot;http://bit.ly/1Jn9Raw&quot;&gt;here&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;View the Ipython Notebook &lt;a href=&quot;http://bit.ly/1Ppo8Z0&quot;&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;a class=&quot;github-button&quot; href=&quot;https://github.com/bikz05/texture-matching&quot; data-icon=&quot;octicon-star&quot; data-style=&quot;mega&quot; aria-label=&quot;Star bikz05/texture-matching on GitHub&quot;&gt;Star&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I hope you liked the post. Thank You for reading.&lt;/p&gt;

&lt;!-- Place this tag right after the last button or just before your close body tag. --&gt;
&lt;script async=&quot;&quot; defer=&quot;&quot; id=&quot;github-bjs&quot; src=&quot;https://buttons.github.io/buttons.js&quot;&gt;&lt;/script&gt;

</description>
        <pubDate>Sat, 30 May 2015 00:00:00 -0700</pubDate>
        <link>http://bikz05.github.io/2015/05/30/local-binary-patterns.html</link>
        <guid isPermaLink="true">http://bikz05.github.io/2015/05/30/local-binary-patterns.html</guid>
        
        <category>python</category>
        
        <category>opencv</category>
        
        <category>local binary patterns</category>
        
        <category>chi-squared distance</category>
        
        
        <category>python</category>
        
        <category>opencv</category>
        
        <category>local binary patterns</category>
        
        <category>chi-squared distance</category>
        
      </item>
    
      <item>
        <title>Digit Recognition using OpenCV, sklearn and Python</title>
        <description>&lt;style&gt;.embed-container { position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; } .embed-container iframe, .embed-container object, .embed-container embed { position: absolute; top: 0; left: 0; width: 100%; height: 100%; }&lt;/style&gt;
&lt;div class=&quot;embed-container&quot;&gt;&lt;iframe src=&quot;http://www.youtube.com/embed/ur6JY2Hl-MM&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Last week, I needed to mail some stuff to one of my friends who recently moved to a new city. So, I called him to inquire about his new address. During the phone call, I quick jotted down the address on a piece of paper and then we took an expected discourse on topics running the entire gamut from movies to politics. After finishing the telephonic conversation, when I gazed at the address that I wrote, it took me a while to understand my own handwriting. I still vividly remember when I was in high school, my mathematics teacher gave me a zero in one of the problems in the test, simply because she was unable to decipher my oracular calligraphy :) , quite an oxymoron . Those were really tough times! Subsequently, I thought about training a classifier to recognize my handwriting. After a couple of days piecemeal work, I was able to recognize my handwriting. Although, I did not do a quantitative analysis of the results, they were all but satisfactory. This motivated me to write a blog post on detecting handwritten digits using HOG features and a multiclass Linear SVM.&lt;/p&gt;

&lt;p&gt;Before we begin, I will succinctly enumerate the steps that are needed to detect handwritten digits -&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Create a database of handwritten digits.&lt;/li&gt;
  &lt;li&gt;For each handwritten digit in the database, extract HOG features and train a Linear SVM.&lt;/li&gt;
  &lt;li&gt; Use the classifier trained in step 2 to predict digits.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;mnistdatabase-of-handwritten-digits&quot;&gt;MNIST database of handwritten digits&lt;/h3&gt;

&lt;p&gt;The first step is to create a database of handwritten digits. We are not going to create a new database but we will use the popular &lt;strong&gt;MNIST database of handwritten digits.&lt;/strong&gt; The MNIST database is a set of 70000 samples of handwritten digits where each sample consists of a grayscale image of size 28×28. There are a total of 70,000 samples. We will use sklearn.datasets package to download the MNIST database from &lt;a href=&quot;mldata.org&quot;&gt;mldata.org&lt;/a&gt;. This package makes it convenient to work with toy datasbases, you can check out the documentation of sklearn.datasets &lt;a href=&quot;http://scikit-learn.org/stable/datasets/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The size of of MNIST database is about 55.4 MB. Once the database is downloaded, it will be cached locally in your hard drive. On my Linux system, by default it is cached in ~/scikit_learn_data/mldata/mnist-original.mat . Alternatively, you can also set the directory where the database will be downloaded.&lt;/p&gt;

&lt;figure id=&quot;figure-1&quot;&gt;&lt;a href=&quot;/figures/mnist-dataset.png&quot;&gt;&lt;img src=&quot;/figures/mnist-dataset.png&quot; alt=&quot;One sample for each handwritten digit in MNSIT database&quot; /&gt;&lt;/a&gt;&lt;figcaption&gt;Figure 1: One sample for each handwritten digit in MNSIT database [&lt;a href=&quot;/figures/mnist-dataset.png&quot;&gt;PNG&lt;/a&gt;]&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;There are approximate 7000 samples for each digit. I actually calculated the number of samples for each digit using collections.Counter class. The actual samples for each digit was -&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Digits&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Number of samples&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;6903&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;7877&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;6990&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;3&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;7141&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;4&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;6824&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;5&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;6313&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;6&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;6876&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;7&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;7293&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;8&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;6825&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;9&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;6958&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;We will write 2 python scripts – one for training the classifier and the second for test the classifier.&lt;/p&gt;

&lt;h3 id=&quot;training-a-classifier&quot;&gt;Training a Classifier&lt;/h3&gt;

&lt;p&gt;Here, we will implement the following steps –&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Calculate the HOG features for each sample in the database.&lt;/li&gt;
  &lt;li&gt;Train a multi-class linear SVM with the HOG features of each sample along with the corresponding label.&lt;/li&gt;
  &lt;li&gt;Save the classifier in a file&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The first step is to import the required modules –&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;lineno&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# Import the modules&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.externals&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;joblib&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;3&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;datasets&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;4&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;skimage.feature&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hog&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;5&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.svm&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;LinearSVC&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;6&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;np&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;We will use the &lt;code&gt;sklearn.externals.joblib&lt;/code&gt; package to save the classifier in a file so that we can use the classifier again without performing training each time. Calculating HOG features for 70000 images is a costly operation, so we will save the classifier in a file and load it whenever we want to use it. As discussed above &lt;code&gt;sklearn.datasets&lt;/code&gt; package will be used to download the MNIST database for handwritten digits. We will use &lt;code&gt;skimage.feature.hog&lt;/code&gt; class to calculate the HOG features and &lt;code&gt;sklearn.svm.LinearSVC&lt;/code&gt; class to perform prediction after training the classifier. We will store our HOG features and labels in numpy arrays. The next step is to download the dataset using the &lt;code&gt;sklearn.datasets.fetch_mldata&lt;/code&gt; function. For the first time, it will take some time as 55.4 MB will be downloaded.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;lineno&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dataset&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;datasets&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fetch_mldata&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;MNIST Original&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Once, the dataset is downloaded we will save the images of the digits in a numpy array &lt;code&gt;features&lt;/code&gt; and the corresponding labels i.e. the digit in another numpy array &lt;code&gt;labels&lt;/code&gt; as shown below –&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;lineno&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;features&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dataset&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;#39;int16&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; 
&lt;span class=&quot;lineno&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dataset&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;target&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;#39;int&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Next, we calculate the HOG features for each image in the database and save them in another numpy array named hog_feature.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;lineno&quot;&gt;17&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;list_hog_fd&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;18&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;feature&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;features&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;19&lt;/span&gt;     &lt;span class=&quot;n&quot;&gt;fd&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hog&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;feature&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;28&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;28&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;orientations&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pixels_per_cell&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;14&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;14&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cells_per_block&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;visualise&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;20&lt;/span&gt;     &lt;span class=&quot;n&quot;&gt;list_hog_fd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;21&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hog_features&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;list_hog_fd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;#39;float64&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In &lt;strong&gt;line 17&lt;/strong&gt; we initialize an empty list &lt;code&gt;list_hog_fd&lt;/code&gt;, where we append the HOG features for each sample in the database. So, in the for loop in &lt;strong&gt;line 18&lt;/strong&gt;, we calculate the HOG features and append them to the list &lt;code&gt;list_hog_fd&lt;/code&gt;. Finally, we create an numpy array &lt;code&gt;hog_features&lt;/code&gt; containing the HOG features which will be used to train the classifier. This step will take some time, so be patient while this piece of code finishes.&lt;/p&gt;

&lt;p&gt;To calculate the HOG features, we set the number of cells in each block equal to one and each individual cell is of size 14×14. Since our image is of size 28×28, we will have four blocks/cells of size 14×14 each. Also, we set the size of orientation vector equal to 9. So our HOG feature vector for each sample will be of size 4×9 = 36. We are not interesting in visualizing the HOG feature image, so we will set the visualise parameter to false.&lt;/p&gt;

&lt;p&gt;If you don’t know about Histogram of Oriented Gaussians (HOG), don’t be disappointed because it is pretty easy to understand. You can check out the below 16 minute &lt;a href=&quot;http://www.youtube.com/watch?v=0Zib1YEE4LU&quot;&gt;YouTube video&lt;/a&gt; by Dr. Mubarak Shah from UCF CRCV. Alternatively, you can check out the documentation of the skimage’s hog function from the &lt;a href=&quot;http://scikit-image.org/docs/dev/auto_examples/plot_hog.html&quot;&gt;official page&lt;/a&gt;. They do discuss tersely about how HOG works.&lt;/p&gt;

&lt;p&gt;The next step is to create a Linear SVM object. Since there are 10 digits, we need a multi-class classifier. The Linear SVM that comes with sklearn can perform multi-class classification.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;lineno&quot;&gt;26&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;clf&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;LinearSVC&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;We preform the training using the fit member function of the &lt;code&gt;clf&lt;/code&gt; object.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;lineno&quot;&gt;29&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;clf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;hog_features&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The &lt;code&gt;fit&lt;/code&gt; function required 2 arguments –one an array of the HOG features of the handwritten digit that we calculated earlier and a corresponding array of labels. Each label value is from the set — &lt;code&gt;[0, 1, 2, 3,…, 8, 9]&lt;/code&gt;. When the training finishes, we will save the classifier in a file named &lt;code&gt;digits_cls.pkl&lt;/code&gt; as shown in the code below -&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;lineno&quot;&gt;32&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;joblib&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dump&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;clf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;digits_cls.pkl&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;compress&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The compress parameter in the &lt;code&gt;joblib.dump&lt;/code&gt; function is used to set how much compression is done and I am quoting this from the documentation –&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;compress: integer for 0 to 9, optional&lt;/p&gt;

  &lt;p&gt;Optional compression level for the data. 0 is no compression. Higher means more compression, but also slower read and write times. Using a value of 3 is often a good compromise.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Up till this point, we have successfully completed the first task of preparing our classifier.&lt;/p&gt;

&lt;h3 id=&quot;testing-the-classifier&quot;&gt;Testing the Classifier&lt;/h3&gt;

&lt;p&gt;Now, we will write another python script to test the classifier. The code for the second script is pretty easy and here is the code for the same –&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;lineno&quot;&gt; 1&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# Import the modules&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt; 2&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;cv2&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt; 3&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.externals&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;joblib&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt; 4&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;skimage.feature&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hog&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt; 5&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt; 6&lt;/span&gt; 
&lt;span class=&quot;lineno&quot;&gt; 7&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# Load the classifier&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt; 8&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;clf&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;joblib&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;load&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;digits_cls.pkl&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt; 9&lt;/span&gt; 
&lt;span class=&quot;lineno&quot;&gt;10&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# Read the input image &lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;11&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;im&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imread&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;/home/bikz05/Desktop/photo8.jpg&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;12&lt;/span&gt; 
&lt;span class=&quot;lineno&quot;&gt;13&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# Convert to grayscale and apply Gaussian filtering&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;14&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;im_gray&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cvtColor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;im&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;COLOR_BGR2GRAY&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;15&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;im_gray&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;GaussianBlur&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;im_gray&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;16&lt;/span&gt; 
&lt;span class=&quot;lineno&quot;&gt;17&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# Threshold the image&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;18&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ret&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;im_th&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;threshold&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;im_gray&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;90&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;255&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;THRESH_BINARY_INV&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;19&lt;/span&gt; 
&lt;span class=&quot;lineno&quot;&gt;20&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# Find contours in the image&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;21&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ctrs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hier&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;findContours&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;im_th&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;copy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;RETR_EXTERNAL&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;CHAIN_APPROX_SIMPLE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;22&lt;/span&gt; 
&lt;span class=&quot;lineno&quot;&gt;23&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# Get rectangles contains each contour&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;24&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rects&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;boundingRect&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ctr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ctr&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ctrs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;25&lt;/span&gt; 
&lt;span class=&quot;lineno&quot;&gt;26&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# For each rectangular region, calculate HOG features and predict&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;27&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# the digit using Linear SVM.&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;28&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rect&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rects&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;29&lt;/span&gt;     &lt;span class=&quot;c&quot;&gt;# Draw the rectangles&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;30&lt;/span&gt;     &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rectangle&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;im&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rect&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rect&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rect&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rect&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rect&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rect&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;255&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; 
&lt;span class=&quot;lineno&quot;&gt;31&lt;/span&gt;     &lt;span class=&quot;c&quot;&gt;# Make the rectangular region around the digit&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;32&lt;/span&gt;     &lt;span class=&quot;n&quot;&gt;leng&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rect&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1.6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;33&lt;/span&gt;     &lt;span class=&quot;n&quot;&gt;pt1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rect&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rect&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;//&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;leng&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;//&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;34&lt;/span&gt;     &lt;span class=&quot;n&quot;&gt;pt2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rect&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rect&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;//&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;leng&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;//&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;35&lt;/span&gt;     &lt;span class=&quot;n&quot;&gt;roi&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;im_th&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pt1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pt1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;leng&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pt2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pt2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;leng&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;36&lt;/span&gt;     &lt;span class=&quot;c&quot;&gt;# Resize the image&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;37&lt;/span&gt;     &lt;span class=&quot;n&quot;&gt;roi&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;resize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;roi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;28&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;28&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;interpolation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;INTER_AREA&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;38&lt;/span&gt;     &lt;span class=&quot;n&quot;&gt;roi&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dilate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;roi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;39&lt;/span&gt;     &lt;span class=&quot;c&quot;&gt;# Calculate the HOG features&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;40&lt;/span&gt;     &lt;span class=&quot;n&quot;&gt;roi_hog_fd&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hog&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;roi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;orientations&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pixels_per_cell&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;14&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;14&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cells_per_block&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;visualise&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;41&lt;/span&gt;     &lt;span class=&quot;n&quot;&gt;nbr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;clf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;roi_hog_fd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;#39;float64&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;42&lt;/span&gt;     &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;putText&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;im&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nbr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rect&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rect&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]),&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;FONT_HERSHEY_DUPLEX&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;255&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;255&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;43&lt;/span&gt; 
&lt;span class=&quot;lineno&quot;&gt;44&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imshow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;Resulting Image with Rectangular ROIs&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;im&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;45&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;waitKey&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;From &lt;strong&gt;line 2-5&lt;/strong&gt; we load the required modules. In &lt;strong&gt;line 8&lt;/strong&gt;, we load the classifier from the file &lt;strong&gt;digits_cls.pkl __which we had saved in the previous script. In __line 11&lt;/strong&gt;, we load the test image and in &lt;strong&gt;line 14&lt;/strong&gt; we convert it to a grayscale image using &lt;code&gt;cv2.cvtColor&lt;/code&gt; function. We then apply a Gaussian filter in &lt;strong&gt;line 15&lt;/strong&gt; to the grayscale image to remove noisy pixels. In &lt;strong&gt;line 18&lt;/strong&gt;, we convert the grayscale image into a binary image using a threshold value of 90. All the pixel locations with grayscale values greater than 90 are set to 0 in the binary image and all the pixel locations with grayscale values less than 90 are set to 255 in the binary image. In &lt;strong&gt;line 21&lt;/strong&gt;, we calculate the contours in the image and then in &lt;strong&gt;line 24&lt;/strong&gt; we calculate the bounding box for each contour. From &lt;strong&gt;line 28-35&lt;/strong&gt; for each bounding box, we generate a bounding square around each contour. Then in &lt;strong&gt;line 37&lt;/strong&gt;, we then resize each bounding square to a size of 28×28 and dilate it in &lt;strong&gt;line 38&lt;/strong&gt;. In &lt;strong&gt;line 40&lt;/strong&gt;, we calculate the HOG features for each bounding square. Remember here that the HOG feature vector for each bounding square should be of the same size for which the classifier was trained, else you will get an error. In &lt;strong&gt;line 41&lt;/strong&gt;, we predict the digit using our classifier. We also draw the bounding box and the predicted digit on the input image. Finally, in &lt;strong&gt;line 44&lt;/strong&gt; we display the image.&lt;/p&gt;

&lt;p&gt;I tested the classifier on this image -&lt;/p&gt;

&lt;figure id=&quot;figure-2&quot;&gt;&lt;a href=&quot;/figures/digit-reco-1-in.jpg&quot;&gt;&lt;img src=&quot;/figures/digit-reco-1-in.jpg&quot; alt=&quot;Input Image&quot; /&gt;&lt;/a&gt;&lt;figcaption&gt;Figure 2: Input Image [&lt;a href=&quot;/figures/digit-reco-1-in.jpg&quot;&gt;JPG&lt;/a&gt;]&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;The resulting image, after running the second script was -&lt;/p&gt;

&lt;figure id=&quot;figure-3&quot;&gt;&lt;a href=&quot;/figures/digit-reco-1-out.png&quot;&gt;&lt;img src=&quot;/figures/digit-reco-1-out.png&quot; alt=&quot;Resultant Image&quot; /&gt;&lt;/a&gt;&lt;figcaption&gt;Figure 3: Resultant Image [&lt;a href=&quot;/figures/digit-reco-1-out.png&quot;&gt;PNG&lt;/a&gt;]&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;So, the results are pretty good.&lt;/p&gt;

&lt;p&gt;Here is another result -&lt;/p&gt;

&lt;figure id=&quot;figure-4&quot;&gt;&lt;a href=&quot;/figures/digit-reco-2.png&quot;&gt;&lt;img src=&quot;/figures/digit-reco-2.png&quot; alt=&quot;All the digits have been correctly recognized.&quot; /&gt;&lt;/a&gt;&lt;figcaption&gt;Figure 4: All the digits have been correctly recognized. [&lt;a href=&quot;/figures/digit-reco-2.png&quot;&gt;PNG&lt;/a&gt;]&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;&lt;em&gt;(Above)&lt;/em&gt; In the image on the left hand side, we display the thresholded image with a square around each digit. Each of this square region is then resized to a 28×28 image. After resizing, we calculate the HOG features of this square region and then using these HOG features we predict the digit. In the image on the right hand side, we display the predicted digit for each handwritten sample bounded in the rectangular box.&lt;/p&gt;

&lt;h3 id=&quot;assumption-during-testing&quot;&gt;Assumption during testing&lt;/h3&gt;

&lt;p&gt;There are a few assumptions, we have assumed in the testing images –&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;The digits should be sufficiently apart from each other. Otherwise if the digits are too close, they will interfere in the square region around each digit. In this case, we will need to create a new square image and then we need to copy the contour in that square image.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;For the images which we used in testing,  fixed thresholding worked pretty well. In most real world images, fixed thresholding does not produce good results. In this case, we need to use adaptive thresholding. You can check out &lt;a href=&quot;http://hanzratech.in/python/adaptive-thresholding/&quot;&gt;this blog post&lt;/a&gt; on adaptive thresholding that I wrote some weeks back.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In the pre-processing step, we only did Gaussian blurring. In most situations, on the binary image we will need to open and close the image to remove small noise pixels and fill small holes.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;recap&quot;&gt;Recap&lt;/h3&gt;

&lt;p&gt;In this tutorial, we discussed how we can recognize handwritten digits using OpenCV, sklearn and Python. We trained a Linear SVM with the HOG features of each sample and tested our code on 2 images. So, That’s it for now!! I hope you liked this blog post.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Thank You&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;get-the-code-img-classemoji-titlesmile-altsmile-srchttpsassetsgithubcomimagesiconsemojiunicode1f604png-height20-width20-alignabsmiddle-&quot;&gt;Get the code &lt;img class=&quot;emoji&quot; title=&quot;:smile:&quot; alt=&quot;:smile:&quot; src=&quot;https://assets.github.com/images/icons/emoji/unicode/1f604.png&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; /&gt;&lt;/h3&gt;

&lt;p&gt;&lt;span class=&quot;bikzclick&quot;&gt;
&lt;a href=&quot;http://bit.ly/1NXUZAY&quot;&gt;Click here to Download the code&lt;/a&gt;
&lt;/span&gt; &lt;/p&gt;

</description>
        <pubDate>Tue, 24 Feb 2015 04:54:14 -0800</pubDate>
        <link>http://bikz05.github.io/2015/02/24/handwritten-digit-recognition-using-opencv-sklearn-and-python.html</link>
        <guid isPermaLink="true">http://bikz05.github.io/2015/02/24/handwritten-digit-recognition-using-opencv-sklearn-and-python.html</guid>
        
        
        <category>python</category>
        
        <category>sklearn</category>
        
        <category>opencv</category>
        
        <category>digit recognition</category>
        
      </item>
    
      <item>
        <title>CAVEAT!! - Thresholding Hue Component</title>
        <description>&lt;p&gt;There are times when we unwittingly commit the same mistake incessantly without actually discovering the reason behind it because we never fathom that something of that sort can actually happen. One of the first hands-on application that fledgling Computer Vision enthusiasts start with is color detection. In color detection, we find the pixel locations whose values match our desired color values (and yes, OpenCV function &lt;a href=&quot;http://docs.opencv.org/modules/core/doc/operations_on_arrays.html#cv2.inRange&quot;&gt;&lt;code&gt;cv2.inRange&lt;/code&gt;&lt;/a&gt; performs this task). In this tutorial, I will discuss about one of the many mistake that most newbies make. I did commit this peccadillo for a long time before I could figure it out. &lt;em&gt;Making mistakes is not a mistake, but not correcting them is definitely.&lt;/em&gt; The philosopher in me, sometimes overpowers the engineer in me. Leaving me aside, in this blog post we will try to detect the Manchester United jersey in the image below using color detection and while doing so we will discover some absurd results. I have chosen the red jersey intentionally and you will figure it out why I did so as we go through this tutorial.&lt;/p&gt;

&lt;figure id=&quot;figure-1&quot;&gt;&lt;a href=&quot;/figures/manu.jpg&quot;&gt;&lt;img src=&quot;/figures/manu.jpg&quot; alt=&quot;In this image, we will detect the red pixels&quot; /&gt;&lt;/a&gt;&lt;figcaption&gt;Figure 1: In this image, we will detect the red pixels [&lt;a href=&quot;/figures/manu.jpg&quot;&gt;JPG&lt;/a&gt;]&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;As you can notice, the jersey is predominantly red, so we will set the cv2.inrange function with the range of RGB values of red color. The code snippet below performs this task -&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;lineno&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;im&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imread&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;manu.jpg&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;im_red_ball_mask&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inRange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;im&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;30&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;80&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;80&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;255&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;3&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;im_red_ball_mask&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;im_red_ball_mask&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;astype&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;#39;bool&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;4&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;myShow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;Resulting image with RGB values within (30-255, 0-80, 0-80)&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;im&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dstack&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;im_red_ball_mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;im_red_ball_mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;im_red_ball_mask&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In &lt;strong&gt;line 1&lt;/strong&gt;, we read the image.The RGB value of true RED color is (255, 0, 0) but in real world images there is always variations in the image color values due to various lightening conditions, shadows and, even due to noise added by the camera while clicking and subsequently processing the image. So, in &lt;strong&gt;line 2&lt;/strong&gt; we set the range of red color values as (30-255, 0-80, 0-80). Any pixel value that lies in between these values is labelled as a red pixel in the output mask &lt;code&gt;im_red_ball_mask&lt;/code&gt;. The mask values are either 255 or 0. 255 represents the red color pixels and 0 represents the pixels that are not red. In &lt;strong&gt;line 3&lt;/strong&gt;, we change the data type of the mask to &lt;strong&gt;boolean&lt;/strong&gt; as it will help us in displaying the image. Finally, in &lt;strong&gt;line 4&lt;/strong&gt;, we display the image using the function named myshow. The myshow function is similar to &lt;code&gt;cv2.imshow&lt;/code&gt; function. Since, I write code in IPython notebooks, I developed my custom display function as the usual &lt;code&gt;cv2.imshow&lt;/code&gt; function does not work in IPython Notebooks. You can replace it by &lt;code&gt;cv2.imshow&lt;/code&gt;, if you are not working in IPython Notebook. This is how the resulting image looks like -&lt;/p&gt;

&lt;figure id=&quot;figure-2&quot;&gt;&lt;a href=&quot;/figures/manu-rgb.png&quot;&gt;&lt;img src=&quot;/figures/manu-rgb.png&quot; alt=&quot;Resulting image after applying the inrange function to get Red Pixel. If look meticulously, you will notice that there are some white pixels that have been incorrectly classified as RGB pixels.&quot; /&gt;&lt;/a&gt;&lt;figcaption&gt;Figure 2: Resulting image after applying the inrange function to get Red Pixel. If look meticulously, you will notice that there are some white pixels that have been incorrectly classified as RGB pixels. [&lt;a href=&quot;/figures/manu-rgb.png&quot;&gt;PNG&lt;/a&gt;]&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;If you look meticulously, you will notice that there are some white pixels that have been incorrectly classified as red pixels. In order to remove these pixels, we will perform color detection in the HSV color space. Let’s convert the image into HSV color space using the &lt;code&gt;cv2.cvtColor&lt;/code&gt; function.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;lineno&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;im_hsv&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cvtColor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;im&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;COLOR_BGR2HSV&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The HSV values for true RED are (0, 255, 255) and to accommodate for variations as discussed above, we will consider a range of HSV values for the red color. So, we will use the &lt;code&gt;cv2.inRange&lt;/code&gt; to generate the mask that has a value of 255 for pixels where the HSV values fall within the range (0-10, 100-255, 0-255)  and a value of 0 for pixels whose values don’t lie in this interval. The mask values are either 255 or 0. 255 represents red color pixels and 0 represents non red color pixels. We will again convert this mask into boolean type and will then display the image using &lt;code&gt;myShow&lt;/code&gt; function.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;lineno&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;im_red_ball_mask_1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inRange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;im_hsv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;255&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;255&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;im_red_ball_mask_1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;im_red_ball_mask_1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;astype&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;#39;bool&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;3&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;myShow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;Resulting image with Hue values within 0-10&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;im&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dstack&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;im_red_ball_mask_1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;im_red_ball_mask_1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;im_red_ball_mask_1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;figure id=&quot;figure-3&quot;&gt;&lt;a href=&quot;/figures/manu_mask_1.png&quot;&gt;&lt;img src=&quot;/figures/manu_mask_1.png&quot; alt=&quot;Resulting image obtained using `cv2.inRange` function with HSV values in the range (0-10, 100-255, 0-255)&quot; /&gt;&lt;/a&gt;&lt;figcaption&gt;Figure 3: Resulting image obtained using `cv2.inRange` function with HSV values in the range (0-10, 100-255, 0-255) [&lt;a href=&quot;/figures/manu_mask_1.png&quot;&gt;PNG&lt;/a&gt;]&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;But what just happened!! Instead of improving, on the contrary our results deteriorated. The resulting image does not contains a major chunk of red pixels that were present in the previous image. The next thought that comes to mind is to tweak the HSV Values, but believe me that would not help. I am not being a pessimist here but there is an underlying idea that needs to be understood before trying anything new. The HSV color space unlike the RGB color space is a cylindrical color space (as shown in the image below). The Hue values are across a circle. So, after completing one rotation across the circle, we get the same color i.e. the Hue values at 0 and 360 (as shown in part &lt;strong&gt;B.&lt;/strong&gt; of the image below) represent the same color which happens to be red. Just to avoid confusion w.r.t. to the diagram below – OpenCV uses HSV ranges between (0-180, 0-255, 0-255), and what you will find in most books and the diagram below is that the range of (0-360, 0-1, 0-1) is used to represent the entire gamut of HSV color space. So in OpenCV, the H values 179, 178, 177 and so on are as close to the true RED as H value 1, 2, 3 and so on.&lt;/p&gt;

&lt;figure id=&quot;figure-4&quot;&gt;&lt;a href=&quot;/figures/hsv_colorspace.jpg&quot;&gt;&lt;img src=&quot;/figures/hsv_colorspace.jpg&quot; alt=&quot;HSV Color Space&quot; /&gt;&lt;/a&gt;&lt;figcaption&gt;Figure 4: HSV Color Space [&lt;a href=&quot;/figures/hsv_colorspace.jpg&quot;&gt;JPG&lt;/a&gt;]&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;So, I think you have grasped the concept quite well and by now, you would have guessed what we will do next. Yes, we will again use the &lt;code&gt;cv2.inRange&lt;/code&gt; function but this time the range of Hue values will be between 170-180 instead of 0-10 used earlier. Here is the code for the new H range values -&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;lineno&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;im_red_ball_mask_2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inRange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;im_hsv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;170&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;180&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;255&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;255&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; 
&lt;span class=&quot;lineno&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;im_red_ball_mask_2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;im_red_ball_mask_2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;astype&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;#39;bool&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;3&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;myShow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;Resulting image with Hue values within 170-180&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;im&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dstack&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;im_red_ball_mask_2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;im_red_ball_mask_2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;im_red_ball_mask_2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The resulting image looks like this -&lt;/p&gt;

&lt;figure id=&quot;figure-5&quot;&gt;&lt;a href=&quot;/figures/manu_mask_2.png&quot;&gt;&lt;img src=&quot;/figures/manu_mask_2.png&quot; alt=&quot;Resulting image obtained using cv2.inRange function with HSV values in the range (170-180, 100-255, 0-255&quot; /&gt;&lt;/a&gt;&lt;figcaption&gt;Figure 5: Resulting image obtained using cv2.inRange function with HSV values in the range (170-180, 100-255, 0-255 [&lt;a href=&quot;/figures/manu_mask_2.png&quot;&gt;PNG&lt;/a&gt;]&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;That’s it, we have successfully recovered our missing red color pixels by using the new Hue values and have also negated the white noise pixels that we had while working with the RGB color space. Now, just for the sake of aesthetics, we will combine both the masks and then we will display the final image.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;lineno&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;im_red_ball_mask_full&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;im_red_ball_mask_1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;im_red_ball_mask_2&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;2&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;myShow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;Resulting image by adding both the masks&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;im&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dstack&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;im_red_ball_mask_full&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;im_red_ball_mask_full&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;im_red_ball_mask_full&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;figure id=&quot;figure-6&quot;&gt;&lt;a href=&quot;/figures/manu_mask_full.png&quot;&gt;&lt;img src=&quot;/figures/manu_mask_full.png&quot; alt=&quot;Resulting image after adding both the masks.&quot; /&gt;&lt;/a&gt;&lt;figcaption&gt;Figure 6: Resulting image after adding both the masks. [&lt;a href=&quot;/figures/manu_mask_full.png&quot;&gt;PNG&lt;/a&gt;]&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h3 id=&quot;recap&quot;&gt;RECAP&lt;/h3&gt;

&lt;p&gt;In this blog post, I showed you how an oversight while setting the Hue values for red color can fail your code. We also discussed how to correctly set the range for hue values for red color. You will need to keep this in mind while working with HSL color space also. I hope you liked this blog post.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Thank You&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;code&quot;&gt;&lt;strong&gt;CODE&lt;/strong&gt;&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;You can test the code in the cloud by importing the &lt;a href=&quot;https://cloud.sagemath.com/projects/04f23e0d-02c9-4862-a739-b02732339e18/files/&quot;&gt;SageMath Cloud project&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;You can also view the code using &lt;a href=&quot;http://nbviewer.ipython.org/gist/bikz05/d352272ac69977f22560&quot;&gt;nbviewer&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

</description>
        <pubDate>Sat, 07 Feb 2015 02:36:56 -0800</pubDate>
        <link>http://bikz05.github.io/2015/02/07/caveat-thresholding-hue-component.html</link>
        <guid isPermaLink="true">http://bikz05.github.io/2015/02/07/caveat-thresholding-hue-component.html</guid>
        
        <category>hsv</category>
        
        <category>thresholding</category>
        
        
        <category>python</category>
        
        <category>opencv</category>
        
        <category>color spaces</category>
        
      </item>
    
      <item>
        <title>Face Recognition using Python and OpenCV</title>
        <description>&lt;style&gt;.embed-container { position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; } .embed-container iframe, .embed-container object, .embed-container embed { position: absolute; top: 0; left: 0; width: 100%; height: 100%; }&lt;/style&gt;
&lt;div class=&quot;embed-container&quot;&gt;&lt;iframe src=&quot;http://www.youtube.com/embed/DP9KX8OaHLw&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Recently, I wanted to perform Face Recognition using OpenCV in Python but sadly, I could not find any good resource for the same. So, after a few hours of work, I wrote my own face recognition program using OpenCV and Python. The actual code is less than 40 lines of python code, thanks to the terse syntax of python and now, I am sharing with you what I did.&lt;/p&gt;

&lt;p&gt;The whole process can be divided in three major steps -&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;The first step is to find a good database of faces with multiple images for each induvidual.&lt;/li&gt;
  &lt;li&gt;The next step is to detect faces in the database images and use them to train the face recognizer.&lt;/li&gt;
  &lt;li&gt;The last step is to test the face recognizer to recognize faces it was trained for.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;So, let’s get started!&lt;/p&gt;

&lt;h1 id=&quot;database&quot;&gt;Database&lt;/h1&gt;

&lt;p&gt;For this tutorial, we will use the &lt;a href=&quot;http://vision.ucsd.edu/content/yale-face-database&quot;&gt;Yale Face Database&lt;/a&gt; that contains 165 grayscale imagesof 15 individuals in gif format, There are 11 images for each individual. In each image, the individual has a different facial expression like happy, sad, normal, surprised, sleepy etc. Indeed, there are 166 images with 12 images for the first individual.&lt;/p&gt;

&lt;figure id=&quot;figure-1&quot;&gt;&lt;a href=&quot;/figures/subject01.jpg&quot;&gt;&lt;img src=&quot;/figures/subject01.jpg&quot; alt=&quot;Set of images for individual number 1. Each image has a different facial expression&quot; /&gt;&lt;/a&gt;&lt;figcaption&gt;Figure 1: Set of images for individual number 1. Each image has a different facial expression [&lt;a href=&quot;/figures/subject01.jpg&quot;&gt;JPG&lt;/a&gt;]&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;We will use this database by using 10 images of the total 11 images of each individual in training our face recognizer and the remaining single image of each individual to test our face recognition algorithm.&lt;/p&gt;

&lt;p&gt;The images corresponding to each individual are named like &lt;code&gt;subject&amp;lt;number&amp;gt;.&amp;lt;facial_expression&amp;gt;&lt;/code&gt; where &lt;code&gt;number&lt;/code&gt; ranges from 01, 02, 03…, 14, 15 and &lt;code&gt;facial_expression&lt;/code&gt; is the expression that the individual has in the image.&lt;/p&gt;

&lt;p&gt;Here is how the images are named for individual numbered 05 -&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;subject05.centerlight
subject05.glasses
subject05.happy
subject05.leftlight
subject05.noglasses
subject05.normal
subject05.rightlight
subject05.sad
subject05.sleepy
subject05.surprised
subject05.wink&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Note, there are 11 images for individual numbered 05. Similarly, for individual numbered 01 -&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;subject01.gif
subject01.glasses
subject01.glasses.gif
subject01.happy
subject01.leftlight
subject01.noglasses
subject01.normal
subject01.rightlight
subject01.sad
subject01.sleepy
subject01.surprised
subject01.wink&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;As I had mentioned earlier, there are 12 images for individual number 01.
Out of the 11 images for each individual (12 for the first individual), we will not train the image with the &lt;strong&gt;.sad&lt;/strong&gt; extension. We will use these images to test the face recognizer.&lt;/p&gt;

&lt;h3 id=&quot;implementation&quot;&gt;Implementation&lt;/h3&gt;

&lt;p&gt;Now, we have an understanding of how our database looks like and it’s time to start programming the face recognition algorithm.&lt;/p&gt;

&lt;h4 id=&quot;import-the-required-modules&quot;&gt;Import the required modules&lt;/h4&gt;

&lt;p&gt;The first step is to import the required modules -&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;code&gt;cv2&lt;/code&gt;  - This is the OpenCV module and contains the functions for face detection and recognition.&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;os&lt;/code&gt;  - This module will be used to maneuver with image and directory names. First, we will use this module to extract the image names in the database directory and then from these names we will extract the individual number, which will be used as a label for the face in that image.&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;Image&lt;/code&gt;  - Since, the dataset images are in gif format and as of now, OpenCV does not support gif format, we will use Image module from PIL  to read the image in grayscale format.&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;numpy&lt;/code&gt;  - Our images will be stored in numpy arrays.&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;lineno&quot;&gt;4&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;os&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;5&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;6&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;PIL&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Image&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h4 id=&quot;load-the-face-detection-cascade&quot;&gt;Load the face detection Cascade&lt;/h4&gt;

&lt;p&gt;The first step is to detect the face in each image. Once, we get the region of interest containing the face in the image, we will use it for training the recognizer. For the purpose of face detection, we will use the Haar Cascade provided by OpenCV. The haar cascades that come with OpenCV are located in the &lt;code&gt;/data/haarcascades&amp;gt;&lt;/code&gt; directory of your OpenCV installation. We will use &lt;code&gt;haarcascade_frontalface_default.xml&lt;/code&gt; for detecting the face. So, we load the cascade using the &lt;code&gt;cv2.CascadeClassifier&lt;/code&gt; function which takes the path to the cascade xml file. I have copied the xml file in the current working directory, so I have used the relative path. In case, you cannot locate the haar cascade file on your computer, I have included it in the zip file available for download at the bottom of the post.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;lineno&quot;&gt; 9&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cascadePath&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;haarcascade_frontalface_default.xml&amp;quot;&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;10&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;faceCascade&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;CascadeClassifier&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cascadePath&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h4 id=&quot;create-the-face-recognizer-object&quot;&gt;Create the Face Recognizer Object&lt;/h4&gt;

&lt;p&gt;The next step is creating the face recognizer object. The face recognizer object has functions like &lt;code&gt;FaceRecognizer.train&lt;/code&gt; to train the recognizer and &lt;code&gt;FaceRecognizer.predict&lt;/code&gt; to recognize a face. OpenCV currently provides 3 face recognizers -&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Eigenface Recognizer - &lt;code&gt;createEigenFaceRecognizer()&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Fisherface Recognizer  - &lt;code&gt;createFisherFaceRecognizer()&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Local Binary Patterns Histograms Face Recognizer - &lt;code&gt;createLBPHFaceRecognizer()&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We will use Local Binary Patterns Histograms Face Recognizer. So, let’s create the face recognizer -&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;recognizer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;createLBPHFaceRecognizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt; - &lt;em&gt;As an aside, if you are interested in knowing how Local Binary Patterns Histograms are created and how they can also be used for texture matching, I would recommend you to go through my blog post on &lt;a href=&quot;/2015/05/30/local-binary-patterns.html&quot;&gt;Texture Matching using Local Binary Patterns (LBP), OpenCV, scikit-learn and Python&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;

&lt;h4 id=&quot;create-the-function-toprepare-the-training-set&quot;&gt;Create the function to prepare the training set&lt;/h4&gt;

&lt;p&gt;Now, we will define a function &lt;code&gt;get_images_and_labels&lt;/code&gt; that takes the absolute path to the image database as input argument and returns tuple of 2 list, one containing the detected faces and the other containing the corresponding label for that face. For example, if the ith index in the list of faces represents the 5th individual in the database, then the corresponding ith location in the list of labels has value equal to 5.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;lineno&quot;&gt;16&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;get_images_and_labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;17&lt;/span&gt;     &lt;span class=&quot;c&quot;&gt;# Append all the absolute image paths in a list image_paths&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;18&lt;/span&gt;     &lt;span class=&quot;c&quot;&gt;# We will not read the image with the .sad extension in the training set&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;19&lt;/span&gt;     &lt;span class=&quot;c&quot;&gt;# Rather, we will use them to test our accuracy of the training&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;20&lt;/span&gt;     &lt;span class=&quot;n&quot;&gt;image_paths&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;join&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;listdir&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;endswith&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;#39;.sad&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;21&lt;/span&gt;     &lt;span class=&quot;c&quot;&gt;# images will contains face images&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;22&lt;/span&gt;     &lt;span class=&quot;n&quot;&gt;images&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;23&lt;/span&gt;     &lt;span class=&quot;c&quot;&gt;# labels will contains the label that is assigned to the image&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;24&lt;/span&gt;     &lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;25&lt;/span&gt;     &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;image_path&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;image_paths&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;26&lt;/span&gt;         &lt;span class=&quot;c&quot;&gt;# Read the image and convert to grayscale&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;27&lt;/span&gt;         &lt;span class=&quot;n&quot;&gt;image_pil&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Image&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;open&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;image_path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;convert&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;#39;L&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;28&lt;/span&gt;         &lt;span class=&quot;c&quot;&gt;# Convert the image format into numpy array&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;29&lt;/span&gt;         &lt;span class=&quot;n&quot;&gt;image&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;image_pil&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;#39;uint8&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;30&lt;/span&gt;         &lt;span class=&quot;c&quot;&gt;# Get the label of the image&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;31&lt;/span&gt;         &lt;span class=&quot;n&quot;&gt;nbr&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;image_path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;.&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;replace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;subject&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;32&lt;/span&gt;         &lt;span class=&quot;c&quot;&gt;# Detect the face in the image&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;33&lt;/span&gt;         &lt;span class=&quot;n&quot;&gt;faces&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;faceCascade&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;detectMultiScale&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;34&lt;/span&gt;         &lt;span class=&quot;c&quot;&gt;# If face is detected, append the face to images and the label to labels&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;35&lt;/span&gt;         &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;faces&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;36&lt;/span&gt;             &lt;span class=&quot;n&quot;&gt;images&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;37&lt;/span&gt;             &lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nbr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;38&lt;/span&gt;             &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imshow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;Adding faces to traning set...&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;39&lt;/span&gt;             &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;waitKey&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;50&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;40&lt;/span&gt;     &lt;span class=&quot;c&quot;&gt;# return the images list and labels list&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;41&lt;/span&gt;     &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;images&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;In &lt;strong&gt;line 19&lt;/strong&gt;, we are appending all the absolute path names of the database images in the list &lt;code&gt;images_path&lt;/code&gt;. We, are not appending images with the &lt;strong&gt;.sad&lt;/strong&gt; extension, as we will use them to test the accuracy of the recognizer. In &lt;strong&gt;line 21 and 23&lt;/strong&gt;, we declare 2 list - images and labels. These are 2 list, that I had discussed in the previous paragraph that will be returned by the recognizer. In the list images, we append the region of interest containing the face and in the list labels, we append the corresponding label for that face. From &lt;strong&gt;line 24 - 38&lt;/strong&gt;, we loop around each image to detect the face in it and update our 2 lists. So, in &lt;strong&gt;line 26 - 28&lt;/strong&gt; we load the current image in a 2D numpy array image. We cannot read the images directly using cv2.imread because as of now, OpenCV doesn’t support gif format images and unfortunately, our database images are in this format. So, we use the Image module from PIL to read the images in grayscale format and convert them into numpy arrays which are compatible with OpenCV. In** line 30&lt;strong&gt;, from the image name, we extract the individual number. This number will be the label for that face. **In line 32&lt;/strong&gt;, we use &lt;code&gt;CascadeClassifier.detectMultiScale&lt;/code&gt; to detect faces in the image. Although, in most cases, we need to tune the  &lt;code&gt;CascadeClassifier.detectMultiScale&lt;/code&gt; function to correctly recognize faces in the image, but for sake of simplicity, I am leaving this part to the default values. You can refer to &lt;a href=&quot;https://realpython.com/blog/python/face-recognition-with-python/&quot;&gt;this &lt;/a&gt; Real Python post for more insights on this. The &lt;code&gt;CascadeClassifier.detectMultiScale&lt;/code&gt; function returns a list of faces. For each face it returns a rectangle in the format &lt;em&gt;(Top-Left x pixel value, Top-Left y pixel value, Width of rectangle, Height of rectangle.)&lt;/em&gt;. In &lt;strong&gt;lines 34-38&lt;/strong&gt;, we slice the ROI from the image and append it to the list &lt;code&gt;images&lt;/code&gt; and the corresponding label in the list &lt;code&gt;labels&lt;/code&gt;. Once, we are done with this loop, we return the 2 lists in the form of a tuple.&lt;/p&gt;

&lt;h4 id=&quot;preparing-the-training-set&quot;&gt;Preparing the training set&lt;/h4&gt;

&lt;p&gt;We pass the &lt;code&gt;get_images_and_labels&lt;/code&gt; function with the &lt;code&gt;path&lt;/code&gt; of the database directory. This path has to be the absolute path. This functions returns the features (images) and labels (labels) which will be used to train the face recognizer in the next step.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;lineno&quot;&gt;42&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# Path to the Yale Dataset&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;43&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;path&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;#39;yalefaces&amp;#39;&lt;/span&gt; 
&lt;span class=&quot;lineno&quot;&gt;44&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# The folder yalefaces is in the same folder as this python script&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;45&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# Call the get_images_and_labels function and get the face images and the &lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;46&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# corresponding labels&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;47&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;images&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;get_images_and_labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;48&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;destroyAllWindows&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;The image below shows that detected faces for each individual. Row 1 contains the face images of individual 1, row 2 contains the face images of individual 2 and so on.&lt;/p&gt;

&lt;figure id=&quot;figure-2&quot;&gt;&lt;a href=&quot;/figures/detectedfaces.png&quot;&gt;&lt;img src=&quot;/figures/detectedfaces.png&quot; alt=&quot;The figure shows the detected faces for each individual. Row 1 contains the face images of individual 1, row 2 contains the face images of individual 2 and so on.&quot; /&gt;&lt;/a&gt;&lt;figcaption&gt;Figure 2: The figure shows the detected faces for each individual. Row 1 contains the face images of individual 1, row 2 contains the face images of individual 2 and so on. [&lt;a href=&quot;/figures/detectedfaces.png&quot;&gt;PNG&lt;/a&gt;]&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h4 id=&quot;perform-the-training&quot;&gt;Perform the training&lt;/h4&gt;

&lt;p&gt;We perform the training using the FaceRecognizer.train function. It requires 2 arguments, the features which in this case are the images of faces and the corresponding labels assigned to these faces which in this case are the induvidual number that we extracted from the image names.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;lineno&quot;&gt;49&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# Perform the training&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;50&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;recognizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;images&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h3 id=&quot;testing-the-face-recognizer&quot;&gt;Testing the face recognizer&lt;/h3&gt;

&lt;p&gt;We will test the results of the recognizer by using the images with .&lt;strong&gt;sad&lt;/strong&gt; extension which we had not used earlier. As done in the &lt;code&gt;get_images_and_labels&lt;/code&gt; function, we append all the image names with the &lt;strong&gt;.sad&lt;/strong&gt; extension in a &lt;code&gt;image_paths&lt;/code&gt; list. Then for each image in the list, we read it in grayscale format and detect faces in it. Once, we have the ROI containing the faces, we pass the ROI to the &lt;code&gt;FaceRecognizer.predict&lt;/code&gt; function which will assign it a label and it will also tell us how confident it is about the recognition. The label is an integer that is one of the individual numbers we had assigned to the faces earler. This label is stored in &lt;code&gt;nbr_predicted&lt;/code&gt;. The more the value of confidence variable is, the less the recognizer has confidence in the recognition. A confidence value of 0.0 is a perfect recognition.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;lineno&quot;&gt;52&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# Append the images with the extension .sad into image_paths&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;53&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;image_paths&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;join&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;listdir&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;endswith&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;#39;.sad&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;54&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;image_path&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;image_paths&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;55&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;predict_image_pil&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Image&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;open&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;image_path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;convert&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;#39;L&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;56&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;predict_image&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predict_image_pil&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;#39;uint8&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;57&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;faces&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;faceCascade&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;detectMultiScale&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predict_image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;58&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;faces&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;59&lt;/span&gt;     &lt;span class=&quot;n&quot;&gt;nbr_predicted&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;conf&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;recognizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predict_image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;60&lt;/span&gt;     &lt;span class=&quot;n&quot;&gt;nbr_actual&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;image_path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;.&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;replace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;subject&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;61&lt;/span&gt;     &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nbr_actual&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nbr_predicted&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;62&lt;/span&gt;         &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;{} is Correctly Recognized with confidence {}&amp;quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nbr_actual&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;conf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;63&lt;/span&gt;     &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;64&lt;/span&gt;         &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;{} is Incorrectly Recognized as {}&amp;quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nbr_actual&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nbr_predicted&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;65&lt;/span&gt;     &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imshow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;Recognizing Face&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;predict_image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;lineno&quot;&gt;66&lt;/span&gt;     &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;waitKey&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;From &lt;strong&gt;line 60-66&lt;/strong&gt;, we check if the recognition was correct by comparing the predicted label &lt;code&gt;nbr_predicted&lt;/code&gt; with the actual label &lt;code&gt;nbr_actual&lt;/code&gt;. The label &lt;code&gt;nbr_actual&lt;/code&gt; is extracted using the os module and the string operations from the name of the image. We also display the confidence score for each recognition.&lt;/p&gt;

&lt;figure id=&quot;figure-3&quot;&gt;&lt;a href=&quot;/figures/facery.png&quot;&gt;&lt;img src=&quot;/figures/facery.png&quot; alt=&quot;Each recognized face with the corresponding confidence.&quot; /&gt;&lt;/a&gt;&lt;figcaption&gt;Figure 3: Each recognized face with the corresponding confidence. [&lt;a href=&quot;/figures/facery.png&quot;&gt;PNG&lt;/a&gt;]&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;In the image above note that, individual number 4 is recognized with a perfect score because &lt;code&gt;subject04.sad&lt;/code&gt; and &lt;code&gt;subject04.normal&lt;/code&gt; are the same images. From the image above, we can see that our Face Recognizer was able to recognize all the faces correctly.&lt;/p&gt;

&lt;h4 id=&quot;recap&quot;&gt;Recap&lt;/h4&gt;

&lt;p&gt;So, in this tutorial we performed the task of face recognition using OpenCV in less than 40 lines of python codes. The code above assigns a label to each image that is to recognized. But, what if the face to be recognized is not even in the database. In that case, the confidence score comes to our rescue. When, the face is not known by the face recognizer, the value of confidence score will be very high and you can use a threshold to ascertain that the face was not recognized.&lt;/p&gt;

&lt;p&gt;I hope you liked this blog post, Thank You, &lt;em&gt;Inquilab Zindabad&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&quot;get-the-code-img-classemoji-titlesmile-altsmile-srchttpsassetsgithubcomimagesiconsemojiunicode1f604png-height20-width20-alignabsmiddle-&quot;&gt;Get the code &lt;img class=&quot;emoji&quot; title=&quot;:smile:&quot; alt=&quot;:smile:&quot; src=&quot;https://assets.github.com/images/icons/emoji/unicode/1f604.png&quot; height=&quot;20&quot; width=&quot;20&quot; align=&quot;absmiddle&quot; /&gt;&lt;/h3&gt;

&lt;p&gt;&lt;span class=&quot;bikzclick&quot;&gt;
&lt;a href=&quot;http://bit.ly/1xpWhdy&quot;&gt;Click here to Download the code&lt;/a&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;h3 id=&quot;resources&quot;&gt;Resources&lt;/h3&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Official OpenCV Documentation - &lt;a href=&quot;http://docs.opencv.org/trunk/modules/contrib/doc/facerec/index.html&quot;&gt;Click here&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Yale Face Database - &lt;a href=&quot;http://vision.ucsd.edu/content/yale-face-database&quot;&gt;Click here&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Tue, 03 Feb 2015 00:00:00 -0800</pubDate>
        <link>http://bikz05.github.io/2015/02/03/face-recognition-using-opencv.html</link>
        <guid isPermaLink="true">http://bikz05.github.io/2015/02/03/face-recognition-using-opencv.html</guid>
        
        <category>computer vision</category>
        
        <category>Face Detection</category>
        
        <category>Face Recognition</category>
        
        <category>image processing</category>
        
        <category>opencv</category>
        
        <category>python</category>
        
        <category>Yale Face Database</category>
        
        
        <category>python</category>
        
        <category>opencv</category>
        
        <category>face recognition</category>
        
        <category>face detection</category>
        
      </item>
    
      <item>
        <title>Setting Up OpenCV library in Eclipse for Android Development</title>
        <description>&lt;p&gt;To develop Android apps using OpenCV Library, we need the following tools (all of these are either Open Source or free software, or both) -&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;http://eclipse%20adt%20link/&quot;&gt;Eclipse with ADT Bundle&lt;/a&gt; (You can choose any other IDE like Android Studio or even the puissant command line!!, but ADT bundle is a must.)&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://sourceforge.net/projects/opencvlibrary/files/opencv-android/&quot;&gt;OpenCV4Android&lt;/a&gt; (although, you can compile from source code, it is recommended for novice users to download the binaries.) &lt;em&gt;Caveat - Download the file named _&lt;strong&gt;OpenCV-2.4.9-android-sdk.zip&lt;/strong&gt;&lt;/em&gt; and not  &lt;em&gt;&lt;strong&gt;opencv-2.4.9.zip&lt;/strong&gt;&lt;/em&gt;. (Version might differ)_&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://%20cygwin.com/install.html&quot;&gt;CygWin&lt;/a&gt; (Only for Windows, not needed on Linux or Mac)&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.oracle.com/technetwork/java/javase/downloads/index.html&quot;&gt;Java JDK&lt;/a&gt; (JRE would not be sufficient)&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://developer.android.com/tools/sdk/ndk/index.html&quot;&gt;Android Native Development Toolkit&lt;/a&gt; (NDK)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You will need to set environment variables on your OS for the tools to correctly configure. For the sake of brevity, I would not discuss each step in detail but for meticulous instructions, refer to the official OpenCV installation &lt;a href=&quot;http://docs.opencv.org/doc/tutorials/introduction/android_binary_package/O4A_SDK.html&quot;&gt;instructions&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Alternatively, nVidia also provides a suite of developing tools — &lt;a href=&quot;http://developer.nvidia.com/tegra-android-development-pack&quot;&gt;TADP&lt;/a&gt;. The advantage of TADP over the piece-by-piece method is that you don’t need to follow the often perplexing task of setting up the development environment. The default installation will download a lot of superfluous packages that are not needed (The download size can be greater than 2GB and on slow internet connections, it can turn into a prolonged activity.), rather select the packages manually at the time of download (A dialog box will appear, asking you to select either Complete, Custom or Manually.)&lt;/p&gt;
</description>
        <pubDate>Sat, 24 Jan 2015 08:45:59 -0800</pubDate>
        <link>http://bikz05.github.io/2015/01/24/setting-up-opencv-library-in-eclipse-for-android-development.html</link>
        <guid isPermaLink="true">http://bikz05.github.io/2015/01/24/setting-up-opencv-library-in-eclipse-for-android-development.html</guid>
        
        
        <category>android</category>
        
        <category>opencv</category>
        
        <category>java</category>
        
        <category>eclipse</category>
        
      </item>
    
      <item>
        <title>Adaptive Thresholding</title>
        <description>&lt;p&gt;One of the most commonly used operation in image processing is thresholding a grayscale image with a fixed value to get a binary image. For example, anything that is greater than 127 in the grayscale, can be set to 1 in the binary image and anything that is less than or equal to 127 in the grayscale image can be set to 0 in the binary image. This process is called &lt;strong&gt;fixed thresholding&lt;/strong&gt; as our threshold value is set to 127.&lt;/p&gt;

&lt;p&gt;In &lt;strong&gt;adaptive threshold&lt;/strong&gt; unlike fixed threshold, the threshold value at each pixel location depends on the neighboring pixel intensities. To calculate the threshold &lt;em&gt;T(x, y)&lt;/em&gt; i.e. the threshold value at pixel location &lt;em&gt;(x, y)&lt;/em&gt; in the image, we perform the following steps -&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;A bxb region around the pixel location is selected. b is selected by the user.&lt;/li&gt;
  &lt;li&gt;The next step is to calculate the weighted average of the bxb region. OpenCV provides 2 methods to calculate this weighted average. We can either use the average (mean) of all the pixel location that lie in the bxb box or we can use a Gaussian weighted average of the pixel values that lie in the box. In the latter case, the pixel values that are near to the center of the box, will have higher weight. We will represent this value by &lt;em&gt;WA(x, y)&lt;/em&gt;.&lt;/li&gt;
  &lt;li&gt;The next step is to find the Threshold value &lt;em&gt;T(x, y)&lt;/em&gt; by subtracting a constant parameter, let’s name it param1 from the weighted average value &lt;em&gt;WA(x, y)&lt;/em&gt; calculated for each pixel in the previous step. The threshold valueT(x, y) at pixel location &lt;em&gt;(x, y)&lt;/em&gt; is then calculated using the formula given below -&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;em&gt;&lt;center&gt;T(x, y) = WA(x, y) - param1&lt;/center&gt;&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;This is how we get our threshold value. Now, it time to program both the fixed and adaptive thresholding in OpenCV. I will be using the IPython environment for this purpose. So, let’s get started.&lt;/p&gt;

&lt;h2 id=&quot;import-the-required-modules&quot;&gt;Import the required modules&lt;/h2&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;o&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pylab&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inline&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;cv2&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;h2 id=&quot;read-the-input-image&quot;&gt;Read the input image&lt;/h2&gt;

&lt;p&gt;First of all read the image, then convert the image to grayscale and finally display the image.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;im&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;imread&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;/home/bikz05/Desktop/image.jpg&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;im_gray&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cvtColor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;im&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;COLOR_RGB2GRAY&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;off&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;Input Image&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;imshow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;im_gray&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cmap&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;#39;gray&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;figure id=&quot;figure-1&quot;&gt;&lt;a href=&quot;/figures/original-thresholding.png&quot;&gt;&lt;img src=&quot;/figures/original-thresholding.png&quot; alt=&quot;Original Image&quot; /&gt;&lt;/a&gt;&lt;figcaption&gt;Figure 1: Original Image [&lt;a href=&quot;/figures/original-thresholding.png&quot;&gt;PNG&lt;/a&gt;]&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h2 id=&quot;fixed-thresholding&quot;&gt;Fixed thresholding&lt;/h2&gt;

&lt;p&gt;We will first perform fixed thresholding on the input image. Fixed Thresholding is done using the function &lt;code&gt;cv2.threshold&lt;/code&gt;. The signature of &lt;code&gt;cv2.threshold&lt;/code&gt; is -&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;threshold&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;src&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;thresh&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;maxval&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dst&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;retval&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dst&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Here,&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;code&gt;src&lt;/code&gt; is the input image.&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;thresh&lt;/code&gt; is the threshold value.&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;maxval&lt;/code&gt; is the maximum value that can be assigned to the output.&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;type&lt;/code&gt; is the type of thresholding.&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;dst&lt;/code&gt; is the destination image&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In the example, the threshold value is set to 50 and the threshold type is &lt;code&gt;cv2.THRESH_BINARY&lt;/code&gt; i.e. any value that is greater than 50 is set to 255 and any value less than 55 is set to 0.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;retval&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;im_at_fixed&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;threshold&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;im_gray&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;50&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;255&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;THRESH_BINARY&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;off&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;Fixed Thresholding&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;imshow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;im_at_fixed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cmap&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;#39;gray&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;figure id=&quot;figure-2&quot;&gt;&lt;a href=&quot;/figures/fixed-thresholding-1.png&quot;&gt;&lt;img src=&quot;/figures/fixed-thresholding-1.png&quot; alt=&quot;Results after fixed thresholding&quot; /&gt;&lt;/a&gt;&lt;figcaption&gt;Figure 2: Results after fixed thresholding [&lt;a href=&quot;/figures/fixed-thresholding-1.png&quot;&gt;PNG&lt;/a&gt;]&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h2 id=&quot;adaptive-thresholding-with-mean-weighted-average&quot;&gt;Adaptive Thresholding with mean weighted average&lt;/h2&gt;

&lt;p&gt;Adaptive Thresholding with mean weighted average is done using the function &lt;code&gt;cv2.adaptiveThreshold&lt;/code&gt;. The signature of the function is -&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;adaptiveThreshold&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;src&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;maxValue&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;adaptiveMethod&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;thresholdType&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;blockSize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;C&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dst&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dst&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Here,&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;code&gt;src&lt;/code&gt;  is the input image.&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;maxval&lt;/code&gt; is the maximum value that can be assigned to the output.&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;adaptiveMethod&lt;/code&gt; is set to &lt;code&gt;cv2.ADAPTIVE_THRESH_MEAN_C&lt;/code&gt;  for mean weighted average and to &lt;code&gt;cv2.ADAPTIVE_THRESH_GAUSSIAN_C&lt;/code&gt; for gaussian weighted average.&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;thresholdType&lt;/code&gt; - the type of thresholding&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;blockSize&lt;/code&gt;  - value of &lt;code&gt;b&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;C&lt;/code&gt; is the constant that is subtracted from the threshold value calculated for each pixel&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;dst&lt;/code&gt; is the destination image&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;For the example, the value of &lt;code&gt;b&lt;/code&gt; is 5 and the constant value is set to 10.&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;im_at_mean&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;adaptiveThreshold&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;im_gray&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;255&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ADAPTIVE_THRESH_MEAN_C&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;THRESH_BINARY&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;off&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;Adaptive Thresholding with mean weighted average&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;imshow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;im_at_mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cmap&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;#39;gray&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;figure id=&quot;figure-3&quot;&gt;&lt;a href=&quot;/figures/adaptive-thresholding-1.png&quot;&gt;&lt;img src=&quot;/figures/adaptive-thresholding-1.png&quot; alt=&quot;Adaptive Thresholding with mean weighted average&quot; /&gt;&lt;/a&gt;&lt;figcaption&gt;Figure 3: Adaptive Thresholding with mean weighted average [&lt;a href=&quot;/figures/adaptive-thresholding-1.png&quot;&gt;PNG&lt;/a&gt;]&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h2 id=&quot;adaptive-thresholding-with-gaussian-weighted-average&quot;&gt;Adaptive Thresholding with gaussian weighted average&lt;/h2&gt;

&lt;p&gt;Adaptive Thresholding with gaussian weighted average is done using the function &lt;code&gt;cv2.adaptiveThreshold&lt;/code&gt;. The signature of the function is -&lt;/p&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;adaptiveThreshold&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;src&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;maxValue&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;adaptiveMethod&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;thresholdType&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;blockSize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;C&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dst&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&amp;gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dst&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Here,&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;code&gt;src&lt;/code&gt; is the input image.&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;maxval&lt;/code&gt; is the maximum value that can be assigned to the output.&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;adaptiveMethod&lt;/code&gt; is set to &lt;code&gt;cv2.ADAPTIVE_THRESH_MEAN_C&lt;/code&gt;  for mean weighted average and to &lt;code&gt;cv2.ADAPTIVE_THRESH_GAUSSIAN_C&lt;/code&gt;  for gaussian weighted average.&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;thresholdType&lt;/code&gt; - the type of thresholding&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;blockSize&lt;/code&gt;  is the value of &lt;code&gt;b&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;C&lt;/code&gt; is the constant that is subtracted from the threshold value calculated for each pixel&lt;/li&gt;
  &lt;li&gt;&lt;code&gt;dst&lt;/code&gt;  is the destination image&lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-python&quot; data-lang=&quot;python&quot;&gt;&lt;span class=&quot;n&quot;&gt;im_at_gauss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;adaptiveThreshold&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;im_gray&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;255&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ADAPTIVE_THRESH_GAUSSIAN_C&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cv2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;THRESH_BINARY&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;off&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;Adaptive Thresholding with gaussian weighted average&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;imshow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;im_at_gauss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cmap&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;#39;gray&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;figure id=&quot;figure-4&quot;&gt;&lt;a href=&quot;/figures/adaptive-thresholding-2.png&quot;&gt;&lt;img src=&quot;/figures/adaptive-thresholding-2.png&quot; alt=&quot;Adaptive Thresholding with gaussian weighted average&quot; /&gt;&lt;/a&gt;&lt;figcaption&gt;Figure 4: Adaptive Thresholding with gaussian weighted average [&lt;a href=&quot;/figures/adaptive-thresholding-2.png&quot;&gt;PNG&lt;/a&gt;]&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;That’s it! Clearly results are better with adaptive thresholding as compared to fixed thresholding.&lt;/p&gt;

&lt;h3 id=&quot;additional-resources&quot;&gt;ADDITIONAL RESOURCES&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://cloud.sagemath.com/projects/bd472b44-f23d-4c37-85e2-627dfeed6883/files/&quot;&gt;Copy the SageMath Project&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://nbviewer.ipython.org/gist/bikz05/323dc9e1167689582626&quot;&gt;View the Notebook using NB Viewer&lt;/a&gt; {GIST DOWNLOAD AVAILABLE HERE]&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Wed, 21 Jan 2015 10:49:40 -0800</pubDate>
        <link>http://bikz05.github.io/2015/01/21/adaptive-thresholding.html</link>
        <guid isPermaLink="true">http://bikz05.github.io/2015/01/21/adaptive-thresholding.html</guid>
        
        <category>adaptiveThreshold</category>
        
        <category>computer vision</category>
        
        <category>image</category>
        
        <category>ipython</category>
        
        <category>opencv</category>
        
        <category>python</category>
        
        <category>threshold</category>
        
        
        <category>python</category>
        
        <category>opencv</category>
        
        <category>adaptive threshold</category>
        
        <category>thresholding</category>
        
      </item>
    
  </channel>
</rss>
